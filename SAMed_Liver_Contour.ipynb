{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/SAM-Liver/blob/main/SAMed_Liver_Contour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SAMed\n",
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics\n",
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "huO1simPGzHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a89ba0-2a07-4262-f834-9ede32cd8040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/151.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/151.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/151.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 16.28 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# this is the small dataset based on L3D_Dataset, change to L3D_Dataset when the code is ready\n",
        "# !gdown --fuzzy https://drive.google.com/file/d/18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz/view?usp=sharing\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1/view?usp=drive_link  #L3D_dataset\n",
        "!ls"
      ],
      "metadata": {
        "id": "5lxkGZBB9-FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471c7fa9-c92c-46a5-825a-0a2e7a2f16d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1\n",
            "To: /content/samed_codes/L3D_Dataset.zip\n",
            "100% 3.64G/3.64G [01:04<00:00, 56.5MB/s]\n",
            "datasets\t preprocess\t\t\t\t sam_lora_image_encoder.py  train.py\n",
            "L3D_Dataset.zip  README.md\t\t\t\t segment_anything\t    utils.py\n",
            "LICENSE\t\t requirements.txt\t\t\t subsample_datasets.py\n",
            "lists\t\t SAMed_h\t\t\t\t test.py\n",
            "materials\t sam_lora_image_encoder_mask_decoder.py  trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
        "!gdown 1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg"
      ],
      "metadata": {
        "id": "_0sdIr0eGnw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d6aaf7-ce74-41af-8a7f-6e62735db590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
            "To: /content/samed_codes/epoch_159.pth\n",
            "100% 19.7M/19.7M [00:00<00:00, 165MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
            "To: /content/samed_codes/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:06<00:00, 61.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q test_dataset.zip\n",
        "!unzip -q L3D_Dataset.zip"
      ],
      "metadata": {
        "id": "3XwGX62q92V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtTdD6SGxBc7",
        "outputId": "7d5927b1-4759-4e0c-a25a-67cf113511a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2 as iio\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import zoom\n",
        "from glob import glob\n",
        "from einops import repeat\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root='/content/samed_codes/test_dataset/Train', low_res=None, transform_img=None, transform_mask=None, istrain=False):\n",
        "        self.img_path_all = glob(root + '/images/*.jpg')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/masks_gt/*.png')  # Update the path and pattern\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "        self.istrain = istrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "        self.brightness = 0.1\n",
        "        self.contrast = 0.1\n",
        "        self.saturation = 0.1\n",
        "        self.hue = 0.1\n",
        "        self.color_aug = transforms.ColorJitter(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_path_all[idx]\n",
        "        mask_path = self.mask_path_all[idx]\n",
        "        # print(f\"Found {len(self.img_path_all)} images and {len(self.mask_path_all)} masks\")\n",
        "\n",
        "        # Open image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L') #29(3), 76(1), 150 (2)\n",
        "        mask = np.array(mask)\n",
        "        # class mapping\n",
        "        mask[mask==0] = 0   #background\n",
        "        mask[mask==76] = 1  #red\n",
        "        mask[mask==150] = 2 #green\n",
        "        mask[mask==29] = 3 #blue\n",
        "        mask = Image.fromarray(mask)\n",
        "        # Apply transformations if provided\n",
        "        if self.istrain:\n",
        "            hflip = random.random() < 0.5\n",
        "            flip_container = random.choice([Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM])\n",
        "            if hflip:\n",
        "                image = self.color_aug(image)\n",
        "                image = image.transpose(flip_container)\n",
        "                mask = mask.transpose(flip_container)\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)  # Convert first channel of mask to Image format\n",
        "        # image = transforms.ToTensor()(image)  # **Converting image to tensor**\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        sample = {'image': image, 'mask': mask}\n",
        "\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(mask, (self.low_res/mask.shape[0], self.low_res/mask.shape[1]), order=0)\n",
        "            sample = {'image': image, 'mask': mask, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "    # transforms.ToTensor(),\n",
        "    ])\n",
        "ds = SegmentationDataset(transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "for sample in ds:\n",
        "    print(np.unique(sample['mask']))  # Check unique values in the mask (class IDs)\n",
        "    plt.subplot(121); plt.imshow(sample['image'].permute(1, 2, 0))  # Rearrange image channels for display\n",
        "    plt.subplot(122); plt.imshow(sample['mask'])  # Display the mask\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "s6BpXai3-SWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd7efae-6d53-4859-d46a-0c62cba5ee68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA"
      ],
      "metadata": {
        "id": "IV6KyKtRIyBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from segment_anything.modeling import Sam\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "class _LoRA_qkv_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            qkv: nn.Module,\n",
        "            linear_a_q: nn.Module,\n",
        "            linear_b_q: nn.Module,\n",
        "            linear_a_v: nn.Module,\n",
        "            linear_b_v: nn.Module,\n",
        "            conv_se_q: nn.Module,\n",
        "            conv_se_v: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = qkv\n",
        "        self.linear_a_q = linear_a_q\n",
        "        self.linear_b_q = linear_b_q\n",
        "        self.linear_a_v = linear_a_v\n",
        "        self.linear_b_v = linear_b_v\n",
        "        self.conv_se_q = conv_se_q\n",
        "        self.conv_se_v = conv_se_v\n",
        "\n",
        "        self.dim = qkv.in_features\n",
        "        self.w_identity = torch.eye(qkv.in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv(x)\n",
        "        a_q_out = self.linear_a_q(x)\n",
        "        a_v_out = self.linear_a_v(x)\n",
        "        a_q_out_temp = self.conv_se_q(a_q_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "        a_v_out_temp = self.conv_se_v(a_v_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "\n",
        "        new_q = self.linear_b_q(torch.mul(a_q_out, torch.sigmoid(a_q_out_temp)))#SE = Squeeze and Excitation\n",
        "        new_v = self.linear_b_v(torch.mul(a_v_out, torch.sigmoid(a_v_out_temp)))\n",
        "\n",
        "        qkv[:, :, :, : self.dim] += new_q\n",
        "        qkv[:, :, :, -self.dim:] += new_v\n",
        "        return qkv\n",
        "\n",
        "class LoRA_Sam_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
        "        super(LoRA_Sam_v0_v2, self).__init__()\n",
        "\n",
        "        assert r > 0\n",
        "        if lora_layer:\n",
        "            self.lora_layer = lora_layer\n",
        "        else:\n",
        "            self.lora_layer = list(\n",
        "                range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
        "        # create for storage, then we can init them or load weights\n",
        "        self.w_As = []  # These are linear layers\n",
        "        self.w_Bs = []\n",
        "\n",
        "        # lets freeze first\n",
        "        for param in sam_model.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Here, we do the surgery\n",
        "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
        "            # If we only want few lora layer instead of all\n",
        "            if t_layer_i not in self.lora_layer:\n",
        "                continue\n",
        "            w_qkv_linear = blk.attn.qkv\n",
        "            self.dim = w_qkv_linear.in_features\n",
        "            w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
        "            w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
        "\n",
        "            conv_se_q = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            conv_se_v = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            self.w_As.append(w_a_linear_q)\n",
        "            self.w_Bs.append(w_b_linear_q)\n",
        "            self.w_As.append(w_a_linear_v)\n",
        "            self.w_Bs.append(w_b_linear_v)\n",
        "            self.w_As.append(conv_se_q)\n",
        "            self.w_As.append(conv_se_v)\n",
        "            blk.attn.qkv = _LoRA_qkv_v0_v2(\n",
        "                w_qkv_linear,\n",
        "                w_a_linear_q,\n",
        "                w_b_linear_q,\n",
        "                w_a_linear_v,\n",
        "                w_b_linear_v,\n",
        "                conv_se_q,\n",
        "                conv_se_v,\n",
        "            )\n",
        "        self.reset_parameters()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def save_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\n",
        "\n",
        "        save both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        num_layer = len(self.w_As)  # actually, it is half\n",
        "        a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
        "        num_layer = len(self.w_Bs)  # actually, it is half\n",
        "        b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
        "        prompt_encoder_tensors = {}\n",
        "        mask_decoder_tensors = {}\n",
        "\n",
        "        # save prompt encoder, only `state_dict`, the `named_parameter` is not permitted\n",
        "        if isinstance(self.sam, torch.nn.DataParallel) or isinstance(self.sam, torch.nn.parallel.DistributedDataParallel):\n",
        "            state_dict = self.sam.module.state_dict()\n",
        "        else:\n",
        "            state_dict = self.sam.state_dict()\n",
        "        for key, value in state_dict.items():\n",
        "            if 'prompt_encoder' in key:\n",
        "                prompt_encoder_tensors[key] = value\n",
        "            if 'mask_decoder' in key:\n",
        "                mask_decoder_tensors[key] = value\n",
        "\n",
        "        merged_dict = {**a_tensors, **b_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}\n",
        "        torch.save(merged_dict, filename)\n",
        "\n",
        "    def load_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\\\n",
        "\n",
        "        load both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        state_dict = torch.load(filename)\n",
        "\n",
        "        for i, w_A_linear in enumerate(self.w_As):\n",
        "            saved_key = f\"w_a_{i:03d}\"\n",
        "            # print('mobarak:', saved_key)\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_A_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        for i, w_B_linear in enumerate(self.w_Bs):\n",
        "            saved_key = f\"w_b_{i:03d}\"\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_B_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        sam_dict = self.sam.state_dict()\n",
        "        sam_keys = sam_dict.keys()\n",
        "\n",
        "        # load prompt encoder\n",
        "        prompt_encoder_keys = [k for k in sam_keys if 'prompt_encoder' in k]\n",
        "        prompt_encoder_values = [state_dict[k] for k in prompt_encoder_keys]\n",
        "        prompt_encoder_new_state_dict = {k: v for k, v in zip(prompt_encoder_keys, prompt_encoder_values)}\n",
        "        sam_dict.update(prompt_encoder_new_state_dict)\n",
        "\n",
        "        # load mask decoder\n",
        "        mask_decoder_keys = [k for k in sam_keys if 'mask_decoder' in k]\n",
        "        mask_decoder_values = [state_dict[k] for k in mask_decoder_keys]\n",
        "        mask_decoder_new_state_dict = {k: v for k, v in zip(mask_decoder_keys, mask_decoder_values)}\n",
        "        sam_dict.update(mask_decoder_new_state_dict)\n",
        "        self.sam.load_state_dict(sam_dict)\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w_A in self.w_As:\n",
        "            nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
        "        for w_B in self.w_Bs:\n",
        "            nn.init.zeros_(w_B.weight)\n",
        "\n",
        "    def forward(self, batched_input, multimask_output, image_size):\n",
        "        return self.sam(batched_input, multimask_output, image_size)"
      ],
      "metadata": {
        "id": "E8zBUeaO8JtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d15267-a28d-4dd3-8130-c77525484134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize & Save Check point"
      ],
      "metadata": {
        "id": "tjKECv_L9plE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_inference(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()  # Disable dropout and batch normalization\n",
        "    num_samples_to_plot = 5  # Number of images to visualize\n",
        "\n",
        "    fig, axs = plt.subplots(num_samples_to_plot, 3, figsize=(12, num_samples_to_plot * 4),\n",
        "                            subplot_kw=dict(xticks=[], yticks=[]))\n",
        "    fig.suptitle(\"Input Image | Predicted Mask | Ground Truth\", fontsize=16)\n",
        "\n",
        "    num_classes = args.num_classes + 1  # Include extra class\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            # Break early if we have plotted enough samples\n",
        "            if i_batch >= num_samples_to_plot:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                image_batch, label_batch, low_res_label_batch = (\n",
        "                    sampled_batch['image'],\n",
        "                    sampled_batch['mask'],\n",
        "                    sampled_batch['low_res_label']\n",
        "                )\n",
        "                image_batch = image_batch.to(device, dtype=torch.float32)\n",
        "                label_batch = label_batch.to(device, dtype=torch.long)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading data for batch {i_batch + 1}: {e}\")\n",
        "                continue  # Skip this batch\n",
        "\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "\n",
        "            # Convert to NumPy arrays for visualization\n",
        "            input_image = image_batch[0].cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
        "            pred_mask = pred_seg[0].cpu().numpy()  # Predicted segmentation\n",
        "            true_mask = label_batch[0].cpu().numpy()  # Ground truth\n",
        "\n",
        "            # Normalize image if needed\n",
        "            input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
        "\n",
        "            # Plot the images\n",
        "            axs[i_batch, 0].imshow(input_image)\n",
        "            axs[i_batch, 0].set_title(\"Input Image\")\n",
        "\n",
        "            axs[i_batch, 1].imshow(pred_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 1].set_title(\"Predicted Mask\")\n",
        "\n",
        "            axs[i_batch, 2].imshow(true_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 2].set_title(\"Ground Truth Mask\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, epoch_since_improvement, best_result, checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'best_loss': best_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        # 'epoch_since_improvement': epoch_since_improvement,  # 儲存 epoch_since_improvement\n",
        "        'best_result': best_result,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} with loss {best_loss} and best epoch {best_epoch}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "  if os.path.isfile(filename):\n",
        "    print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "    checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    best_loss = checkpoint.get('best_loss', np.inf)\n",
        "    best_epoch = checkpoint.get('best_epoch', 0)\n",
        "    # epoch_since_improvement = checkpoint.get('epoch_since_improvement', 0)\n",
        "    best_result = checkpoint.get('best_result', None)\n",
        "\n",
        "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint.get('epoch', 'N/A')))  # load epoch\n",
        "\n",
        "    # return start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result\n",
        "    return start_epoch, best_loss, best_epoch, best_result\n",
        "  else:\n",
        "    print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "    return 1, [0, 0], 0, 0, None"
      ],
      "metadata": {
        "id": "xR2tnwt49sr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM Training"
      ],
      "metadata": {
        "id": "2XLzKt8YaiAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adjust_learning_rate(optimizer, iter_num, args):\n",
        "    if args.warmup and iter_num < args.warmup_period:\n",
        "        lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "    else:\n",
        "        if args.warmup:\n",
        "            shift_iter = iter_num - args.warmup_period\n",
        "            assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "        else:\n",
        "            shift_iter = iter_num\n",
        "        lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "    return lr_\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def inference_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    # fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(output, label_batch, ce_loss, dice_loss, args):\n",
        "    # print(\"label_batch\",label_batch.shape)\n",
        "    # print(output.shape)\n",
        "    loss_ce = ce_loss(output, label_batch[:].long())\n",
        "    loss_dice = dice_loss(output, label_batch, softmax=True)\n",
        "    loss = (1 - args.dice_weight) * loss_ce + args.dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "        outputs = model(image_batch, multimask_output, args.img_size)\n",
        "        output = outputs[args.output_key]\n",
        "        loss_label_batch = batch_dict[args.batch_key]\n",
        "        loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update learning rate and increment iteration count\n",
        "        lr_current = adjust_learning_rate(optimizer, iter_num, args)\n",
        "        iter_num += 1\n",
        "\n",
        "        loss_all.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(loss_all), iter_num, lr_current\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            batch_dict = {'image_batch':image_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            output = outputs[args.output_key]\n",
        "            loss_label_batch = batch_dict[args.batch_key]\n",
        "            loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Add new arguments\n",
        "    parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "    parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "    parser.add_argument('--weights', type=int, nargs='+', default=None,help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/L3D_Dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='L3D_Dataset')    ##### remember to change to L3D when the code is working\n",
        "    # parser.add_argument('--data_path', type=str, default='Endonasal_Slices_Voxel')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--output_file', type=str, default='model_best.pt') ############\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int, default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=6, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=30, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/' + args.output_file\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    # pkg = import_module(args.module)\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    # net.load_lora_parameters(args.lora_ckpt)\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "\n",
        "    transform_img = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    transform_mask = transforms.Compose([\n",
        "        transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "        # transforms.ToTensor(),\n",
        "        ])\n",
        "# ds = SegmentationDataset(transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "\n",
        "    train_dataset = SegmentationDataset(root=(args.data_path+'/Train'), low_res=128, transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    # # test if there is low_res_label\n",
        "    # for i_batch, sampled_batch in enumerate(trainloader):\n",
        "    #   print(f\"Sampled batch keys: {sampled_batch.keys()}\")  # 確認是否有 'low_res_label'\n",
        "\n",
        "\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "    ######### load checkpoint\n",
        "    # start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "    start_epoch, best_loss, best_epoch, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "\n",
        "    if start_epoch == 1 and isinstance(best_loss, list) and best_loss == [0, 0]:\n",
        "        print(\"No checkpoint loaded; starting training from scratch.\")\n",
        "        best_loss = np.inf\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} with best loss {best_loss}, best epoch {best_epoch}.\")\n",
        "    ########\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        loss_training, iter_num, lr_current = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        ##### save checkpoint\n",
        "        checkpoint_path = '/content/samed_codes/checkpoint.pth'\n",
        "        # save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, epoch_since_improvement, best_result, checkpoint_path)\n",
        "        save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, best_result, checkpoint_path)\n",
        "        #####\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            net.save_lora_parameters(os.path.join(args.output_dir, args.output_file))\n",
        "\n",
        "        print('--- Epoch {}/{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "    format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, lr_current))\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = inference_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))\n",
        "    plot_inference(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None)  # visualize\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # safe checkpoint\n",
        "    checkpoint_path = 'checkpoints'\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    main()"
      ],
      "metadata": {
        "id": "36CLN8Rj7MgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b620616-d8ae-4d64-9e86-52bc4a0068d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/samed_codes/segment_anything/build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda train sample size: 921 test sample size: 109 batch: 6\n",
            "--- Epoch 0/30: Training loss = 0.6136, Testing: [loss = 0.6029, dice = 0.3165], Best loss = 0.6029, Best epoch = 0, lr = 0.003080\n",
            "--- Epoch 1/30: Training loss = 0.5984, Testing: [loss = 0.6020, dice = 0.3389], Best loss = 0.6020, Best epoch = 1, lr = 0.004991\n",
            "--- Epoch 2/30: Training loss = 0.5924, Testing: [loss = 0.5939, dice = 0.3374], Best loss = 0.5939, Best epoch = 2, lr = 0.004968\n",
            "--- Epoch 3/30: Training loss = 0.5911, Testing: [loss = 0.5858, dice = 0.3365], Best loss = 0.5858, Best epoch = 3, lr = 0.004945\n",
            "--- Epoch 4/30: Training loss = 0.5918, Testing: [loss = 0.5883, dice = 0.3316], Best loss = 0.5858, Best epoch = 3, lr = 0.004922\n",
            "--- Epoch 5/30: Training loss = 0.5920, Testing: [loss = 0.5864, dice = 0.3423], Best loss = 0.5858, Best epoch = 3, lr = 0.004899\n",
            "--- Epoch 6/30: Training loss = 0.5904, Testing: [loss = 0.5868, dice = 0.3373], Best loss = 0.5858, Best epoch = 3, lr = 0.004876\n",
            "--- Epoch 7/30: Training loss = 0.5944, Testing: [loss = 0.5872, dice = 0.3430], Best loss = 0.5858, Best epoch = 3, lr = 0.004853\n",
            "--- Epoch 8/30: Training loss = 0.5925, Testing: [loss = 0.5913, dice = 0.3356], Best loss = 0.5858, Best epoch = 3, lr = 0.004829\n",
            "--- Epoch 9/30: Training loss = 0.5901, Testing: [loss = 0.5892, dice = 0.3295], Best loss = 0.5858, Best epoch = 3, lr = 0.004806\n",
            "--- Epoch 10/30: Training loss = 0.5906, Testing: [loss = 0.5902, dice = 0.3349], Best loss = 0.5858, Best epoch = 3, lr = 0.004783\n",
            "--- Epoch 11/30: Training loss = 0.5900, Testing: [loss = 0.5883, dice = 0.3323], Best loss = 0.5858, Best epoch = 3, lr = 0.004760\n",
            "--- Epoch 12/30: Training loss = 0.5891, Testing: [loss = 0.5894, dice = 0.3377], Best loss = 0.5858, Best epoch = 3, lr = 0.004737\n",
            "--- Epoch 13/30: Training loss = 0.5890, Testing: [loss = 0.5883, dice = 0.3338], Best loss = 0.5858, Best epoch = 3, lr = 0.004713\n",
            "--- Epoch 14/30: Training loss = 0.5889, Testing: [loss = 0.5855, dice = 0.3378], Best loss = 0.5855, Best epoch = 14, lr = 0.004690\n",
            "--- Epoch 15/30: Training loss = 0.5892, Testing: [loss = 0.5856, dice = 0.3384], Best loss = 0.5855, Best epoch = 14, lr = 0.004667\n",
            "--- Epoch 16/30: Training loss = 0.5889, Testing: [loss = 0.5899, dice = 0.3399], Best loss = 0.5855, Best epoch = 14, lr = 0.004644\n",
            "--- Epoch 17/30: Training loss = 0.5887, Testing: [loss = 0.5903, dice = 0.3289], Best loss = 0.5855, Best epoch = 14, lr = 0.004620\n",
            "--- Epoch 18/30: Training loss = 0.5892, Testing: [loss = 0.5881, dice = 0.3348], Best loss = 0.5855, Best epoch = 14, lr = 0.004597\n",
            "--- Epoch 19/30: Training loss = 0.5881, Testing: [loss = 0.5878, dice = 0.3390], Best loss = 0.5855, Best epoch = 14, lr = 0.004574\n",
            "--- Epoch 20/30: Training loss = 0.5891, Testing: [loss = 0.5889, dice = 0.3417], Best loss = 0.5855, Best epoch = 14, lr = 0.004550\n",
            "--- Epoch 21/30: Training loss = 0.5881, Testing: [loss = 0.5884, dice = 0.3321], Best loss = 0.5855, Best epoch = 14, lr = 0.004527\n",
            "--- Epoch 22/30: Training loss = 0.5877, Testing: [loss = 0.5863, dice = 0.3382], Best loss = 0.5855, Best epoch = 14, lr = 0.004504\n",
            "--- Epoch 23/30: Training loss = 0.5882, Testing: [loss = 0.5849, dice = 0.3435], Best loss = 0.5849, Best epoch = 23, lr = 0.004480\n",
            "--- Epoch 24/30: Training loss = 0.5876, Testing: [loss = 0.5867, dice = 0.3378], Best loss = 0.5849, Best epoch = 23, lr = 0.004457\n",
            "--- Epoch 25/30: Training loss = 0.5883, Testing: [loss = 0.5863, dice = 0.3391], Best loss = 0.5849, Best epoch = 23, lr = 0.004433\n",
            "--- Epoch 26/30: Training loss = 0.5873, Testing: [loss = 0.5890, dice = 0.3337], Best loss = 0.5849, Best epoch = 23, lr = 0.004410\n",
            "--- Epoch 27/30: Training loss = 0.5870, Testing: [loss = 0.5846, dice = 0.3452], Best loss = 0.5846, Best epoch = 27, lr = 0.004387\n",
            "--- Epoch 28/30: Training loss = 0.5869, Testing: [loss = 0.5835, dice = 0.3417], Best loss = 0.5835, Best epoch = 28, lr = 0.004363\n",
            "--- Epoch 29/30: Training loss = 0.5871, Testing: [loss = 0.5887, dice = 0.3375], Best loss = 0.5835, Best epoch = 28, lr = 0.004340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-84fc6d378b6b>:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(filename)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Wise Dice: {'dice_cls:1': 0, 'dice_cls:2': 0, 'dice_cls:3': 0}\n",
            "Overall Dice: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cqs19OGiPWfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}