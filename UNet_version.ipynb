{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/SAM-Liver/blob/main/UNet_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2M4gZbKZWt"
      },
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3D1PuuLQMm"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HmmYvx7FLUif",
        "outputId": "07893210-6b19-45d0-9555-e3e56dc8e877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.8/89.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tSMFkgPhyc"
      },
      "source": [
        "# Download codes, pretrained weights and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RyB2eYACPtEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050acfe6-0921-414f-9637-8adfe13cd5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 225 (delta 42), reused 29 (delta 29), pack-reused 167 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 636.92 KiB | 20.54 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ]
        }
      ],
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --fuzzy https://drive.google.com/file/d/18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz/view?usp=sharing\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1/view?usp=drive_link  #L3D_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XeWl16zq0XO",
        "outputId": "df864297-6e1e-4f4f-a53a-64a3f9b2e0a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1\n",
            "From (redirected): https://drive.google.com/uc?id=1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1&confirm=t&uuid=92a6dd1b-5522-429c-87df-8953d281b95a\n",
            "To: /content/samed_codes/samed_codes/L3D_Dataset.zip\n",
            "100% 3.64G/3.64G [01:18<00:00, 46.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1fAoOVHvAxPh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "2cc986e7-16b8-4dfc-86bd-7cb46be0e561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'L3D_Dataset.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e312a6fe1320>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L3D_Dataset.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# with zipfile.ZipFile('test_dataset.zip', 'r') as zip_ref:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'L3D_Dataset.zip'"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "#dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('L3D_Dataset.zip', 'r') as zip_ref:\n",
        "# with zipfile.ZipFile('test_dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "#weights\n",
        "!gdown https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr #'epoch_159.pth'\n",
        "!gdown https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg #'sam_vit_b_01ec64.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQmJASbCqUT"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w27C6DKCvOK"
      },
      "outputs": [],
      "source": [
        "%cd /content/samed_codes\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from glob import glob\n",
        "import imageio.v2 as iio\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "# def map_labels(label):\n",
        "#     label_map = {0: 0, 85: 1, 128:0, 170: 2, 255: 3}\n",
        "#     mapped_label = label.copy()\n",
        "#     for k, v in label_map.items():\n",
        "#         mapped_label[label == k] = v\n",
        "#     return mapped_label\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root='/content/samed_codes/test_dataset/Train', low_res=None, transform_img=None, transform_mask=None, istrain=False):\n",
        "        self.img_path_all = glob(root + '/images/*.jpg')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/masks_gt/*.png')  # Update the path and pattern\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "        self.istrain = istrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "        self.brightness = 0.1\n",
        "        self.contrast = 0.1\n",
        "        self.saturation = 0.1\n",
        "        self.hue = 0.1\n",
        "        self.color_aug = transforms.ColorJitter(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_path_all[idx]\n",
        "        mask_path = self.mask_path_all[idx]\n",
        "        # print(f\"Found {len(self.img_path_all)} images and {len(self.mask_path_all)} masks\")\n",
        "\n",
        "        # Open image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L') #29(3), 76(1), 150 (2)\n",
        "        mask = np.array(mask)\n",
        "        # class mapping\n",
        "        mask[mask==0] = 0   #background\n",
        "        mask[mask==76] = 1  #red\n",
        "        mask[mask==150] = 2 #green\n",
        "        mask[mask==29] = 3 #blue\n",
        "        mask = Image.fromarray(mask)\n",
        "        # Apply transformations if provided\n",
        "        if self.istrain:\n",
        "            hflip = random.random() < 0.5\n",
        "            flip_container = random.choice([Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM])\n",
        "            if hflip:\n",
        "                image = self.color_aug(image)\n",
        "                image = image.transpose(flip_container)\n",
        "                mask = mask.transpose(flip_container)\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)  # Convert first channel of mask to Image format\n",
        "        # image = transforms.ToTensor()(image)  # **Converting image to tensor**\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        sample = {'image': image, 'mask': mask}\n",
        "\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(mask, (self.low_res/mask.shape[0], self.low_res/mask.shape[1]), order=0)\n",
        "            sample = {'image': image, 'mask': mask, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_dataset = SegmentationDataset(root='/content/samed_codes/test_dataset/Train', low_res=128)\n",
        "test_dataset = SegmentationDataset(root='/content/samed_codes/test_dataset/Test', low_res=128)\n",
        "print('Train Sample:', len(train_dataset), 'Test Sample:', len(test_dataset))\n",
        "sample = train_dataset[3]\n",
        "\n",
        "input, label, low_res_label = np.array(sample['image']), sample['mask'], sample['low_res_label']\n",
        "print('Unique classes:', np.unique(low_res_label))\n",
        "print('min-mas:',input.min(), input.max())\n",
        "\n",
        "################\n",
        "print(\"Original shape:\", input.shape)\n",
        "print(\"After transpose:\", input.transpose(1, 2, 0).shape)\n",
        "\n",
        "################\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "plt.subplot(1,4,1), plt.axis('OFF'), plt.title('in:{}'.format(input.shape)), plt.imshow(input)\n",
        "plt.subplot(1,4,2), plt.axis('OFF'), plt.title('in:{}'.format(input[0].shape)), plt.imshow(input[0])\n",
        "plt.subplot(1,4,3), plt.axis('OFF'), plt.title('lab:{}'.format(label.shape)), plt.imshow(label);\n",
        "plt.subplot(1,4,4), plt.axis('OFF'), plt.title('low:{}'.format(low_res_label.shape)), plt.imshow(low_res_label);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize & Save Check point"
      ],
      "metadata": {
        "id": "eZ7qOva2Czzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, best_result, checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'best_loss': best_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        # 'epoch_since_improvement': epoch_since_improvement,\n",
        "        'best_result': best_result,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at '{checkpoint_path}' (Epoch {epoch}, Best Loss {best_loss}, Best Epoch {best_epoch})\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename, device='cpu'):\n",
        "    if os.path.isfile(filename):  # 檢查檔案是否存在\n",
        "        print(f\"=> Loading checkpoint from '{filename}'\")\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "        # 載入模型和優化器的狀態\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # 獲取其他保存的狀態\n",
        "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
        "        best_epoch = checkpoint.get('best_epoch', 0)\n",
        "        best_result = checkpoint.get('best_result', None)\n",
        "\n",
        "        print(f\"=> Loaded checkpoint (Epoch {checkpoint.get('epoch', 'N/A')})\")\n",
        "        return start_epoch, best_loss, best_epoch, best_result\n",
        "    else:\n",
        "        print(f\"=> No checkpoint found at '{filename}'\")\n",
        "        # 返回初始狀態，確保不會中斷訓練流程\n",
        "        return 1, float('inf'), 0, None"
      ],
      "metadata": {
        "id": "-WrXG7fiBl3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_inference(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()  # Disable dropout and batch normalization\n",
        "    num_samples_to_plot = 3  # Number of images to visualize\n",
        "\n",
        "    fig, axs = plt.subplots(num_samples_to_plot, 3, figsize=(12, num_samples_to_plot * 4),\n",
        "                            subplot_kw=dict(xticks=[], yticks=[]))\n",
        "    fig.suptitle(\"Input Image | Predicted Mask | Ground Truth\", fontsize=16)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            if i_batch >= num_samples_to_plot:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                image_batch, label_batch, low_res_label_batch = (\n",
        "                    sampled_batch['image'],\n",
        "                    sampled_batch['mask'],\n",
        "                    sampled_batch['low_res_label']\n",
        "                )\n",
        "                image_batch = image_batch.to(device, dtype=torch.float32)\n",
        "                label_batch = label_batch.to(device, dtype=torch.long)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading data for batch {i_batch + 1}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Call the model with only the image_batch\n",
        "            outputs = model(image_batch)  # Removed multimask_output and args.img_size\n",
        "            logits = outputs  # Assuming model directly outputs logits\n",
        "            pred_seg = torch.argmax(logits, dim=1)\n",
        "\n",
        "            input_image = image_batch[0].cpu().numpy().transpose(1, 2, 0)\n",
        "            pred_mask = pred_seg[0].cpu().numpy()\n",
        "            true_mask = label_batch[0].cpu().numpy()\n",
        "\n",
        "            input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
        "\n",
        "            axs[i_batch, 0].imshow(input_image)\n",
        "            axs[i_batch, 0].set_title(\"Input Image\")\n",
        "\n",
        "            axs[i_batch, 1].imshow(pred_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 1].set_title(\"Predicted Mask\")\n",
        "\n",
        "            axs[i_batch, 2].imshow(true_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 2].set_title(\"Ground Truth Mask\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6ada7AwlBgib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j2gsPPfB45E"
      },
      "source": [
        "UNet Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install icecream medpy monai"
      ],
      "metadata": {
        "id": "WtFZjhZdBml7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXw9Wa1dDFET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea03b88-7899-4fb8-c2b4-66285c8aedf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n",
            "Training on: cuda train sample size: 12 test sample size: 12 batch: 8\n",
            "=> no checkpoint found at 'checkpoint.pth'\n",
            "No checkpoint loaded; starting training from scratch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 0\\10: Training loss = 0.9179, Testing: [loss = 0.9343, dice = 0.1682], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 1\\10: Training loss = 0.9179, Testing: [loss = 0.9346, dice = 0.1681], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 2\\10: Training loss = 0.9196, Testing: [loss = 0.9349, dice = 0.1680], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 3\\10: Training loss = 0.9182, Testing: [loss = 0.9352, dice = 0.1678], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 4\\10: Training loss = 0.9158, Testing: [loss = 0.9355, dice = 0.1677], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 5\\10: Training loss = 0.9183, Testing: [loss = 0.9358, dice = 0.1676], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 6\\10: Training loss = 0.9168, Testing: [loss = 0.9358, dice = 0.1676], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 7\\10: Training loss = 0.9171, Testing: [loss = 0.9359, dice = 0.1676], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 8\\10: Training loss = 0.9180, Testing: [loss = 0.9360, dice = 0.1675], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n",
            "--- Epoch 9\\10: Training loss = 0.9177, Testing: [loss = 0.9362, dice = 0.1674], Best loss = 0.9343, Best epoch = 0, lr = 0.000004\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "import monai\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(outputs, label_batch, ce_loss, dice_loss, dice_weight:float=0.8):\n",
        "    loss_ce = ce_loss(outputs, label_batch[:].long())\n",
        "    loss_dice = dice_loss(outputs, label_batch, softmax=True)\n",
        "    loss = (1 - dice_weight) * loss_ce + dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        outputs = model(image_batch)\n",
        "        # print('out:',outputs.shape)\n",
        "        loss, loss_ce, loss_dice = calc_loss(outputs, label_batch, ce_loss, dice_loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all.append(loss.item())\n",
        "        if args.warmup and iter_num < args.warmup_period:\n",
        "            lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "        else:\n",
        "            if args.warmup:\n",
        "                shift_iter = iter_num - args.warmup_period\n",
        "                assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "            else:\n",
        "                shift_iter = iter_num\n",
        "            lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "\n",
        "        iter_num = iter_num + 1\n",
        "\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    #####\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/L3D_Dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='test_dataset')\n",
        "    #####\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=3, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=50, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "\n",
        "    net = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "                         in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "    net.conv = nn.Conv2d(32, args.num_classes + 1, kernel_size=1, stride=1)\n",
        "    net.to(device)\n",
        "    ########\n",
        "    transform_img = transforms.Compose([\n",
        "      transforms.Resize((512, 512)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std=[0.229, 0.224, 0.225]\n",
        "          )\n",
        "      ])\n",
        "    transform_mask = transforms.Compose([\n",
        "        transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "        # transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "    train_dataset = SegmentationDataset(root=(args.data_path+'/Train'), low_res=128, transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "\n",
        "    ######### load chechpoint\n",
        "    checkpoint_path = '/content/samed_codes/checkpoint.pth'\n",
        "    start_epoch, best_loss, best_epoch, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "    # start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "\n",
        "    if start_epoch == 1 and isinstance(best_loss, list) and best_loss == [0, 0]:\n",
        "        print(\"No checkpoint loaded; starting training from scratch.\")\n",
        "        best_loss = np.inf\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} with best loss {best_loss}, best epoch {best_epoch}.\")\n",
        "    ########\n",
        "    print ('Test data numbers: ', len(train_dataset), 'Train data numbers: ', len(test_dataset))\n",
        "\n",
        "    for epoch in range(start_epoch, args.max_epochs):\n",
        "        loss_training = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        ##### save checkpoint\n",
        "        save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, best_result, checkpoint_path)\n",
        "        #####\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            torch.save(net.state_dict(), os.path.join(args.output_dir, 'model_best.pt'))\n",
        "            # net.save_lora_parameters(os.path.join(args.output_dir, 'model_best.pt'))\n",
        "\n",
        "        print('--- Epoch {}\\{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "            format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, optimizer.param_groups[0]['lr']))\n",
        "        # Visualize\n",
        "        if epoch % 5 ==0:\n",
        "          plot_inference(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # safe checkpoint\n",
        "    checkpoint_path = 'checkpoints'\n",
        "\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupTnYetNuNe"
      },
      "source": [
        "Inference: My Dice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJEAXtUcLSVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "outputId": "0c9be1a9-e4ed-40fc-8daf-afb7d2f822ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-2c4aea7e344f>:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(args.ckpt))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"model.0.conv.unit0.conv.weight\", \"model.0.conv.unit0.conv.bias\", \"model.0.conv.unit0.adn.A.weight\", \"model.0.conv.unit1.conv.weight\", \"model.0.conv.unit1.conv.bias\", \"model.0.conv.unit1.adn.A.weight\", \"model.0.residual.weight\", \"model.0.residual.bias\", \"model.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.0.conv.unit1.adn.A.weight\", \"model.1.submodule.0.residual.weight\", \"model.1.submodule.0.residual.bias\", \"model.1.submodule.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\", \"model.1.submodule.1.submodule.0.residual.weight\", \"model.1.submodule.1.submodule.0.residual.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.0.residual.weight\", \"model.1.submodule.1.submodule.1.submodule.0.residual.bias\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.residual.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.residual.bias\", \"model.1.submodule.1.submodule.1.submodule.2.0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.2.0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.2.0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.2.0.conv.weight\", \"model.1.submodule.1.submodule.2.0.conv.bias\", \"model.1.submodule.1.submodule.2.0.adn.A.weight\", \"model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\", \"model.1.submodule.2.0.conv.weight\", \"model.1.submodule.2.0.conv.bias\", \"model.1.submodule.2.0.adn.A.weight\", \"model.1.submodule.2.1.conv.unit0.conv.weight\", \"model.1.submodule.2.1.conv.unit0.conv.bias\", \"model.1.submodule.2.1.conv.unit0.adn.A.weight\", \"model.2.0.conv.weight\", \"model.2.0.conv.bias\", \"model.2.0.adn.A.weight\", \"model.2.1.conv.unit0.conv.weight\", \"model.2.1.conv.unit0.conv.bias\". \n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encoder4.enc4norm2.running_mean\", \"encoder4.enc4norm2.running_var\", \"encoder4.enc4norm2.num_batches_tracked\", \"bottleneck.bottleneckconv1.weight\", \"bottleneck.bottlenecknorm1.weight\", \"bottleneck.bottlenecknorm1.bias\", \"bottleneck.bottlenecknorm1.running_mean\", \"bottleneck.bottlenecknorm1.running_var\", \"bottleneck.bottlenecknorm1.num_batches_tracked\", \"bottleneck.bottleneckconv2.weight\", \"bottleneck.bottlenecknorm2.weight\", \"bottleneck.bottlenecknorm2.bias\", \"bottleneck.bottlenecknorm2.running_mean\", \"bottleneck.bottlenecknorm2.running_var\", \"bottleneck.bottlenecknorm2.num_batches_tracked\", \"upconv4.weight\", \"upconv4.bias\", \"decoder4.dec4conv1.weight\", \"decoder4.dec4norm1.weight\", \"decoder4.dec4norm1.bias\", \"decoder4.dec4norm1.running_mean\", \"decoder4.dec4norm1.running_var\", \"decoder4.dec4norm1.num_batches_tracked\", \"decoder4.dec4conv2.weight\", \"decoder4.dec4norm2.weight\", \"decoder4.dec4norm2.bias\", \"decoder4.dec4norm2.running_mean\", \"decoder4.dec4norm2.running_var\", \"decoder4.dec4norm2.num_batches_tracked\", \"upconv3.weight\", \"upconv3.bias\", \"decoder3.dec3conv1.weight\", \"decoder3.dec3norm1.weight\", \"decoder3.dec3norm1.bias\", \"decoder3.dec3norm1.running_mean\", \"decoder3.dec3norm1.running_var\", \"decoder3.dec3norm1.num_batches_tracked\", \"decoder3.dec3conv2.weight\", \"decoder3.dec3norm2.weight\", \"decoder3.dec3norm2.bias\", \"decoder3.dec3norm2.running_mean\", \"decoder3.dec3norm2.running_var\", \"decoder3.dec3norm2.num_batches_tracked\", \"upconv2.weight\", \"upconv2.bias\", \"decoder2.dec2conv1.weight\", \"decoder2.dec2norm1.weight\", \"decoder2.dec2norm1.bias\", \"decoder2.dec2norm1.running_mean\", \"decoder2.dec2norm1.running_var\", \"decoder2.dec2norm1.num_batches_tracked\", \"decoder2.dec2conv2.weight\", \"decoder2.dec2norm2.weight\", \"decoder2.dec2norm2.bias\", \"decoder2.dec2norm2.running_mean\", \"decoder2.dec2norm2.running_var\", \"decoder2.dec2norm2.num_batches_tracked\", \"upconv1.weight\", \"upconv1.bias\", \"decoder1.dec1conv1.weight\", \"decoder1.dec1norm1.weight\", \"decoder1.dec1norm1.bias\", \"decoder1.dec1norm1.running_mean\", \"decoder1.dec1norm1.running_var\", \"decoder1.dec1norm1.num_batches_tracked\", \"decoder1.dec1conv2.weight\", \"decoder1.dec1norm2.weight\", \"decoder1.dec1norm2.bias\", \"decoder1.dec1norm2.running_mean\", \"decoder1.dec1norm2.running_var\", \"decoder1.dec1norm2.num_batches_tracked\", \"conv.weight\", \"conv.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2c4aea7e344f>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mdice_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_dice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdices_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultimask_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"model.0.conv.unit0.conv.weight\", \"model.0.conv.unit0.conv.bias\", \"model.0.conv.unit0.adn.A.weight\", \"model.0.conv.unit1.conv.weight\", \"model.0.conv.unit1.conv.bias\", \"model.0.conv.unit1.adn.A.weight\", \"model.0.residual.weight\", \"model.0.residual.bias\", \"model.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.0.conv.unit1.adn.A.weight\", \"model.1.submodule.0.residual.weight\", \"model.1.submodule.0.residual.bias\", \"model.1.submodule.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\", \"model.1.submodule.1.submodule.0.residual.weight\", \"model.1.submodule.1.submodule.0.residual.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\"...\n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encod..."
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            logits = model(image_batch)\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            loss_dice = dice_loss(logits, label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')  ###############\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'results/model_best.pt'\n",
        "    net = monai.networks.nets.UNet(\n",
        "        spatial_dims=2,\n",
        "        in_channels=3,\n",
        "        out_channels=args.num_classes + 1,\n",
        "        channels=(16, 32, 64, 128, 256),\n",
        "        strides=(2, 2, 2, 2),\n",
        "        num_res_units=2,\n",
        "    ).to(device)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    net.load_state_dict(torch.load(args.ckpt))\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference MRI to MRI<br>\n",
        "download test mri"
      ],
      "metadata": {
        "id": "OeaaFJN0iUvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gdown\n",
        "# url = 'https://drive.google.com/uc?id=1zcvnBscFVI2v5ieAlGGnp0zpX8WmIrD4'\n",
        "# gdown.download(url,'endonasal_mri_patients.zip',quiet=True)\n",
        "# !unzip -q endonasal_mri_patients\n",
        "# !rm -rf /content/endonasal_mri_patients/.DS_Store\n",
        "# !rm -rf /content/endonasal_mri_patients/**/.DS_Store"
      ],
      "metadata": {
        "id": "u55aRX3KjbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRI to MRI"
      ],
      "metadata": {
        "id": "3mLQJNoU0rId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import nibabel as nib\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# from scipy.ndimage import zoom\n",
        "# from einops import repeat\n",
        "# import random\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# def read_mri(mri_path):\n",
        "#     img_meta = nib.load(mri_path)\n",
        "#     array = img_meta.get_fdata()\n",
        "#     return np.rot90(array)\n",
        "\n",
        "# def normalise_intensity(image, ROI_thres=0.1):\n",
        "#     pixel_thres = np.percentile(image, ROI_thres)\n",
        "#     ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "#     mean = np.mean(ROI)\n",
        "#     std = np.std(ROI)\n",
        "#     ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "#     return ROI_norm\n",
        "\n",
        "# class EndonasalDataset_MRI(Dataset):\n",
        "#     def __init__(self, root='endonasal_mri_patients', patient=None, low_res=None, isTrain=False):\n",
        "\n",
        "#         mri_path = 'endonasal_mri_patients/mri0{}/mri0{}_t1c.nii.gz'.format(patient, patient)\n",
        "#         self.mask_path = 'endonasal_mri_patients/mri0{}/mri0{}_mask.nii.gz'.format(patient, patient)\n",
        "#         mri_array = read_mri(mri_path)\n",
        "#         self.image_all = []\n",
        "#         for z in range(mri_array.shape[2]):\n",
        "#             normalized_slice = cv2.normalize(mri_array[:, :, z], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "#             self.image_all.append(normalise_intensity(normalized_slice))\n",
        "\n",
        "#         self.mask_all = read_mri(self.mask_path)\n",
        "#         self.isTrain = isTrain\n",
        "#         self.low_res = low_res\n",
        "\n",
        "#     def __len__(self):\n",
        "#       return len(self.image_all)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         image = self.image_all[index]\n",
        "#         image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "#         label = self.mask_all[:,:,index]\n",
        "#         label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "#         if self.isTrain:\n",
        "#             if random.random() > 0.5:\n",
        "#                 image, label = random_rot_flip(image, label)\n",
        "#             elif random.random() > 0.5:\n",
        "#                 image, label = random_rotate(image, label)\n",
        "\n",
        "#         image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "#         sample = {'image': image, 'label': label}\n",
        "#         if self.low_res:\n",
        "#             low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "#             sample = {'image': image, 'label': label, 'low_res_label': low_res_label, 'maskpath': self.mask_path}\n",
        "\n",
        "#         return sample\n",
        "\n",
        "\n",
        "# def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "#     replace_indices = np.vstack((\n",
        "#         ground_truth.flatten(),\n",
        "#         prediction.flatten())\n",
        "#     ).T\n",
        "#     confusion_matrix, _ = np.histogramdd(\n",
        "#         replace_indices,\n",
        "#         bins=(nr_labels, nr_labels),\n",
        "#         range=[(0, nr_labels), (0, nr_labels)]\n",
        "#     )\n",
        "#     confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "#     return confusion_matrix\n",
        "\n",
        "# def calculate_dice(confusion_matrix):\n",
        "#     dices = []\n",
        "#     for index in range(confusion_matrix.shape[0]):\n",
        "#         true_positives = confusion_matrix[index, index]\n",
        "#         false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "#         false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "#         denom = 2 * true_positives + false_positives + false_negatives\n",
        "#         if denom == 0:\n",
        "#             dice = 0\n",
        "#         else:\n",
        "#             dice = 2 * float(true_positives) / denom\n",
        "#         dices.append(dice)\n",
        "#     return dices\n",
        "\n",
        "# def pred_to_mri(pred_seg_all, mask_path):\n",
        "#     os.makedirs('predicted_mri', mode = 0o777, exist_ok = True)\n",
        "#     img_meta = nib.load(mask_path)\n",
        "#     pred_seg_all = np.rot90(np.array(pred_seg_all).transpose(1,2,0), k=-1)\n",
        "#     img_nifti = nib.Nifti1Image(pred_seg_all, img_meta.affine, header=img_meta.header)\n",
        "#     nib.save(img_nifti,'predicted_mri/'+os.path.basename(mask_path))\n",
        "\n",
        "# def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "#     model.eval()\n",
        "#     loss_per_epoch, dice_per_epoch = [], []\n",
        "#     num_classes = args.num_classes + 1\n",
        "#     confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "#     class_wise_dice = []\n",
        "#     pred_seg_all = []\n",
        "#     with torch.no_grad():\n",
        "#         for i_batch, sampled_batch in enumerate(testloader):\n",
        "#             image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "#             image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "#             outputs = model(image_batch, multimask_output, args.img_size)\n",
        "#             logits = outputs['masks']\n",
        "#             prob = F.softmax(logits, dim=1)\n",
        "#             pred_seg = torch.argmax(prob, dim=1)\n",
        "#             pred_seg_all.extend(pred_seg.detach().cpu().numpy())\n",
        "#             confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "#             loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "#             loss_per_epoch.append(loss.item())\n",
        "#             dice_per_epoch.append(1-loss_dice.item())\n",
        "#             low_res_logits = outputs['low_res_logits']\n",
        "#             loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "#             metric_list = []\n",
        "#             pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "#         pred_to_mri(np.array(pred_seg_all), sampled_batch['maskpath'][0])\n",
        "#         confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "#         dices_per_class = {'dice_cls:{}'.format(cls + 1): round(dice, 4)\n",
        "#                     for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "#     return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "#     parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "#     parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "#     parser.add_argument('--num_classes', type=int, default=2)\n",
        "#     parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "#     parser.add_argument('--output_dir', type=str, default='results')\n",
        "#     parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "#     parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "#     parser.add_argument('--seed', type=int,\n",
        "#                         default=1234, help='random seed')\n",
        "#     parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "#     parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "#     parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "#                         help='Pretrained checkpoint')\n",
        "#     parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "#     parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "#     parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "#     parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "#     parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "#     parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "#     parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "#     parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "#     parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "#     parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "#     parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "#     if 'ipykernel' in sys.modules:\n",
        "#         args = parser.parse_args([])\n",
        "#     else:\n",
        "#         args = parser.parse_args()\n",
        "\n",
        "#     args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "#     args.lora_ckpt = 'results/model_best.pt'\n",
        "#     sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "#                                                                     num_classes=args.num_classes,\n",
        "#                                                                     checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "#                                                                     pixel_std=[1, 1, 1])\n",
        "\n",
        "#     pkg = import_module(args.module)\n",
        "#     net = pkg.LoRA_Sam(sam, args.rank).cuda()\n",
        "#     ce_loss = CrossEntropyLoss()\n",
        "#     dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "#     assert args.lora_ckpt is not None\n",
        "#     net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "#     patients = ['154', '169', '170']\n",
        "#     mean_overall = []\n",
        "#     tumor_overall = []\n",
        "#     carotid_overall = []\n",
        "#     for patient in patients:\n",
        "#         test_dataset = EndonasalDataset_MRI(root='endonasal_mri_patients', patient=patient, low_res=128)\n",
        "#         testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "#         test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "#         dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "#         overall = round(np.mean(dices_per_class_list),4)\n",
        "#         mean_overall.append(overall)\n",
        "#         tumor_overall.append(dices_per_class['dice_cls:1'])\n",
        "#         carotid_overall.append(dices_per_class['dice_cls:2'])\n",
        "#         print('Patient:', patient, ',Class Wise:', dices_per_class, ',Overall :', overall)\n",
        "\n",
        "#     print('Overall Model Performance [Mean Overall]:', round(np.mean(mean_overall),4), '[cls-1:{}]'.format(round(np.mean(tumor_overall),4)),\\\n",
        "#             '[cls-2:{}]'.format(round(np.mean(carotid_overall),4)))\n"
      ],
      "metadata": {
        "id": "IJKKAr8UiTry",
        "outputId": "1bbecab0-dff9-4478-d4f4-6c5916cb1ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient: 154 ,Class Wise: {'dice_cls:1': 0.9737, 'dice_cls:2': 0.7039} ,Overall : 0.8388\n",
            "Patient: 169 ,Class Wise: {'dice_cls:1': 0.991, 'dice_cls:2': 0.9621} ,Overall : 0.9766\n",
            "Patient: 170 ,Class Wise: {'dice_cls:1': 0.9894, 'dice_cls:2': 0.8928} ,Overall : 0.9411\n",
            "Overall Model Performance [Mean Overall]: 0.9188 [cls-1:0.9847] [cls-2:0.8529]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post Processing to remove outliers (small seg)"
      ],
      "metadata": {
        "id": "_dBB8gMP8yHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from skimage import morphology\n",
        "# import nibabel as nib\n",
        "# mask_meta = nib.load('/content/samed_codes/endonasal_mri_patients/mri0169/mri0169_mask.nii.gz')\n",
        "# mask = nib.load('/content/samed_codes/predicted_mri/mri0169_mask.nii.gz').get_fdata()\n",
        "\n",
        "# binary_mask = morphology.remove_small_objects(mask>0, 50)\n",
        "# mask[binary_mask==0] = 0\n",
        "# img_nifti = nib.Nifti1Image(mask, mask_meta.affine, header=mask_meta.header)\n",
        "# nib.save(img_nifti,'mri0169_mask_post_processed.nii.gz')"
      ],
      "metadata": {
        "id": "c8y3CsOW1O6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er8Wnp5M85Zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}