{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/SAM-Liver/blob/main/UNet_version_small_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2M4gZbKZWt"
      },
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3D1PuuLQMm"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmmYvx7FLUif",
        "outputId": "38578deb-7825-4a1e-a098-08959d691d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.8/89.8 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tSMFkgPhyc"
      },
      "source": [
        "# Download codes, pretrained weights and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyB2eYACPtEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e409c1-2d23-4af8-d3c3-cf60d8aaca5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 225 (delta 42), reused 29 (delta 29), pack-reused 167 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 636.92 KiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ]
        }
      ],
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --fuzzy https://drive.google.com/file/d/18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz/view?usp=sharing\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1/view?usp=drive_link  #L3D_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XeWl16zq0XO",
        "outputId": "422dd3a9-c9ec-4080-ef3e-2d6d6382c3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz\n",
            "From (redirected): https://drive.google.com/uc?id=18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz&confirm=t&uuid=d0667656-8eb7-437b-89c8-0bc57a3f7f67\n",
            "To: /content/samed_codes/test_dataset.zip\n",
            "100% 139M/139M [00:04<00:00, 32.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fAoOVHvAxPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8318168-c6e3-4428-8692-1eca42fe8d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
            "To: /content/samed_codes/epoch_159.pth\n",
            "100% 19.7M/19.7M [00:00<00:00, 247MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
            "From (redirected): https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg&confirm=t&uuid=4687e62f-8433-40e8-84c0-a1d9be71b93c\n",
            "To: /content/samed_codes/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:06<00:00, 57.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "#dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('L3D_Dataset.zip', 'r') as zip_ref:\n",
        "# with zipfile.ZipFile('test_dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "#weights\n",
        "!gdown https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr #'epoch_159.pth'\n",
        "!gdown https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg #'sam_vit_b_01ec64.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQmJASbCqUT"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8w27C6DKCvOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d61bfd0-62bd-4aaf-eb80-81ee246a22f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2 as iio\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import zoom\n",
        "from glob import glob\n",
        "from einops import repeat\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root='/content/samed_codes/L3D_Dataset/Train', low_res=None, transform_img=None, transform_mask=None, istrain=False):\n",
        "        self.img_path_all = glob(root + '/images/*.jpg')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/masks_gt/*.png')  # Update the path and pattern\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "        self.istrain = istrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "        self.brightness = 0.1\n",
        "        self.contrast = 0.1\n",
        "        self.saturation = 0.1\n",
        "        self.hue = 0.1\n",
        "        self.color_aug = transforms.ColorJitter(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_path_all[idx]\n",
        "        mask_path = self.mask_path_all[idx]\n",
        "        # print(f\"Found {len(self.img_path_all)} images and {len(self.mask_path_all)} masks\")\n",
        "\n",
        "        # Open image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L') #29(3), 76(1), 150 (2)\n",
        "        mask = np.array(mask)\n",
        "        # class mapping\n",
        "        mask[mask==0] = 0   #background\n",
        "        mask[mask==76] = 1  #red\n",
        "        mask[mask==150] = 2 #green\n",
        "        mask[mask==29] = 3 #blue\n",
        "        mask = Image.fromarray(mask)\n",
        "        # Apply transformations if provided\n",
        "        if self.istrain:\n",
        "            hflip = random.random() < 0.5\n",
        "            flip_container = random.choice([Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM])\n",
        "            if hflip:\n",
        "                image = self.color_aug(image)\n",
        "                image = image.transpose(flip_container)\n",
        "                mask = mask.transpose(flip_container)\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)  # Convert first channel of mask to Image format\n",
        "        # image = transforms.ToTensor()(image)  # **Converting image to tensor**\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        sample = {'image': image, 'mask': mask}\n",
        "\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(mask, (self.low_res/mask.shape[0], self.low_res/mask.shape[1]), order=0)\n",
        "            sample = {'image': image, 'mask': mask, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "    # transforms.ToTensor(),\n",
        "    ])\n",
        "ds = SegmentationDataset(transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "for sample in ds:\n",
        "    print(np.unique(sample['mask']))  # Check unique values in the mask (class IDs)\n",
        "    plt.subplot(121); plt.imshow(sample['image'].permute(1, 2, 0))  # Rearrange image channels for display\n",
        "    plt.subplot(122); plt.imshow(sample['mask'])  # Display the mask\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize & Save Check point"
      ],
      "metadata": {
        "id": "eZ7qOva2Czzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, best_result, checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'best_loss': best_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        # 'epoch_since_improvement': epoch_since_improvement,\n",
        "        'best_result': best_result,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at '{checkpoint_path}' (Epoch {epoch}, Best Loss {best_loss}, Best Epoch {best_epoch})\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename, device='cpu'):\n",
        "    if os.path.isfile(filename):  # 檢查檔案是否存在\n",
        "        print(f\"=> Loading checkpoint from '{filename}'\")\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "        # 載入模型和優化器的狀態\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # 獲取其他保存的狀態\n",
        "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
        "        best_epoch = checkpoint.get('best_epoch', 0)\n",
        "        best_result = checkpoint.get('best_result', None)\n",
        "\n",
        "        print(f\"=> Loaded checkpoint (Epoch {checkpoint.get('epoch', 'N/A')})\")\n",
        "        return start_epoch, best_loss, best_epoch, best_result\n",
        "    else:\n",
        "        print(f\"=> No checkpoint found at '{filename}'\")\n",
        "        # 返回初始狀態，確保不會中斷訓練流程\n",
        "        return 1, float('inf'), 0, None\n",
        "\n"
      ],
      "metadata": {
        "id": "-WrXG7fiBl3G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_inference(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()  # Disable dropout and batch normalization\n",
        "    num_samples_to_plot = 3  # Number of images to visualize\n",
        "\n",
        "    fig, axs = plt.subplots(num_samples_to_plot, 3, figsize=(12, num_samples_to_plot * 4),\n",
        "                            subplot_kw=dict(xticks=[], yticks=[]))\n",
        "    fig.suptitle(\"Input Image | Predicted Mask | Ground Truth\", fontsize=16)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            if i_batch >= num_samples_to_plot:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                image_batch, label_batch, low_res_label_batch = (\n",
        "                    sampled_batch['image'],\n",
        "                    sampled_batch['mask'],\n",
        "                    sampled_batch['low_res_label']\n",
        "                )\n",
        "                image_batch = image_batch.to(device, dtype=torch.float32)\n",
        "                label_batch = label_batch.to(device, dtype=torch.long)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading data for batch {i_batch + 1}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Call the model with only the image_batch\n",
        "            outputs = model(image_batch)  # Removed multimask_output and args.img_size\n",
        "            logits = outputs  # Assuming model directly outputs logits\n",
        "            pred_seg = torch.argmax(logits, dim=1)\n",
        "\n",
        "            input_image = image_batch[0].cpu().numpy().transpose(1, 2, 0)\n",
        "            pred_mask = pred_seg[0].cpu().numpy()\n",
        "            true_mask = label_batch[0].cpu().numpy()\n",
        "\n",
        "            input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
        "\n",
        "            axs[i_batch, 0].imshow(input_image)\n",
        "            axs[i_batch, 0].set_title(\"Input Image\")\n",
        "\n",
        "            axs[i_batch, 1].imshow(pred_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 1].set_title(\"Predicted Mask\")\n",
        "\n",
        "            axs[i_batch, 2].imshow(true_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 2].set_title(\"Ground Truth Mask\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3em5euxBTLGo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j2gsPPfB45E"
      },
      "source": [
        "UNet Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install monai medpy icecream"
      ],
      "metadata": {
        "id": "Mgplvprm2g_M",
        "outputId": "169fa6cb-1dd6-4b4d-fbe6-eabebc2a5e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting medpy\n",
            "  Downloading medpy-0.5.2.tar.gz (156 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting icecream\n",
            "  Downloading icecream-2.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting numpy<2.0,>=1.24 (from monai)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from medpy) (1.14.1)\n",
            "Collecting SimpleITK>=2.1 (from medpy)\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting colorama>=0.3.9 (from icecream)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from icecream) (2.18.0)\n",
            "Collecting executing>=2.1.0 (from icecream)\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->monai)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\n",
            "Downloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading icecream-2.1.4-py3-none-any.whl (14 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.5.2-py3-none-any.whl size=224709 sha256=0f1544141c007f1903eeea94e15218bbe3b77b18e3525c00ab75e3613cabe8c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/33/ed/aaac5a347fb8d41679ca515b8f5c49dfdf49be15bdbb9a905d\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, executing, colorama, asttokens, nvidia-cusparse-cu12, nvidia-cudnn-cu12, icecream, nvidia-cusolver-cu12, medpy, monai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed SimpleITK-2.4.1 asttokens-3.0.0 colorama-0.4.6 executing-2.2.0 icecream-2.1.4 medpy-0.5.2 monai-1.4.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MXw9Wa1dDFET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6905b4-19ca-48f3-b65c-d784552e7b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "import monai\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(outputs, label_batch, ce_loss, dice_loss, dice_weight:float=0.8):\n",
        "    loss_ce = ce_loss(outputs, label_batch[:].long())\n",
        "    loss_dice = dice_loss(outputs, label_batch, softmax=True)\n",
        "    loss = (1 - dice_weight) * loss_ce + dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        outputs = model(image_batch)\n",
        "        # print('out:',outputs.shape)\n",
        "        loss, loss_ce, loss_dice = calc_loss(outputs, label_batch, ce_loss, dice_loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all.append(loss.item())\n",
        "        if args.warmup and iter_num < args.warmup_period:\n",
        "            lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "        else:\n",
        "            if args.warmup:\n",
        "                shift_iter = iter_num - args.warmup_period\n",
        "                assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "            else:\n",
        "                shift_iter = iter_num\n",
        "            lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "\n",
        "        iter_num = iter_num + 1\n",
        "\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch = []\n",
        "    dice_per_class = np.zeros(args.num_classes)  # save dice per class\n",
        "    count_per_class = np.zeros(args.num_classes)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, _ = sampled_batch['image'], sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(image_batch)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "\n",
        "            # calaulate dice for each class\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            for cls in range(args.num_classes):\n",
        "                mask_pred = (preds == cls).float()\n",
        "                mask_true = (label_batch == cls).float()\n",
        "                intersection = (mask_pred * mask_true).sum()\n",
        "                union = mask_pred.sum() + mask_true.sum()\n",
        "                if union > 0:\n",
        "                    dice_cls = (2. * intersection) / union\n",
        "                    dice_per_class[cls] += dice_cls.item()\n",
        "                    count_per_class[cls] += 1\n",
        "\n",
        "    # calculate average dice\n",
        "    avg_dice_per_class = dice_per_class / np.maximum(count_per_class, 1)\n",
        "\n",
        "    return np.mean(loss_per_epoch), avg_dice_per_class\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    #####\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/test_dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='test_dataset')\n",
        "    #####\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=50, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    ############# ask!!\n",
        "    net = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "                         in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "    net.conv = nn.Conv2d(32, args.num_classes + 1, kernel_size=1, stride=1)\n",
        "    # net.conv = nn.Conv2d(32, args.num_classes, kernel_size=1, stride=1)\n",
        "    net.to(device)\n",
        "    ############\n",
        "    transform_img = transforms.Compose([\n",
        "      transforms.Resize((512, 512)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std=[0.229, 0.224, 0.225]\n",
        "          )\n",
        "      ])\n",
        "    transform_mask = transforms.Compose([\n",
        "        transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "        # transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "    train_dataset = SegmentationDataset(root=(args.data_path+'/Train'), low_res=128, transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "    ####\n",
        "    # for i_batch, sampled_batch in enumerate(trainloader):\n",
        "    #   label_batch = sampled_batch['mask']  # 假設 'mask' 是標籤\n",
        "    #   unique_classes = torch.unique(label_batch)\n",
        "      # print(f\"Batch {i_batch} unique classes: {unique_classes.tolist()}\")\n",
        "    # 檢查標籤張量中包含哪些類別\n",
        "\n",
        "    ####\n",
        "    print(f\"Number of classes: {args.num_classes}\")\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    # dice_loss = DiceLoss(args.num_classes )\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "\n",
        "    ######### load chechpoint\n",
        "    checkpoint_path = '/content/samed_codes/checkpoint.pth'\n",
        "    start_epoch, best_loss, best_epoch, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "    # start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "\n",
        "    if start_epoch == 1 and isinstance(best_loss, list) and best_loss == [0, 0]:\n",
        "        print(\"No checkpoint loaded; starting training from scratch.\")\n",
        "        best_loss = np.inf\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} with best loss {best_loss}, best epoch {best_epoch}.\")\n",
        "    ########\n",
        "    print ('Test data numbers: ', len(train_dataset), 'Train data numbers: ', len(test_dataset))\n",
        "\n",
        "    for epoch in range(start_epoch, args.max_epochs):\n",
        "        loss_training = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        ##### save checkpoint\n",
        "        save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, best_result, checkpoint_path)\n",
        "        #####\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            torch.save(net.state_dict(), os.path.join(args.output_dir, 'model_best.pt'))\n",
        "            # net.save_lora_parameters(os.path.join(args.output_dir, 'model_best.pt'))\n",
        "\n",
        "        print(f'--- Epoch {epoch}/{args.max_epochs}: Training loss = {loss_training:.4f}, Testing: [loss = {loss_testing:.4f}]')\n",
        "        for cls in range(0, args.num_classes):  # 動態根據類別數設定\n",
        "          print(f'    Dice for class {cls}: {dice_per_class[cls-1]}')\n",
        "        # Visualize\n",
        "        if epoch % 15 ==0:\n",
        "          plot_inference(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # safe checkpoint\n",
        "    checkpoint_path = 'checkpoints'\n",
        "\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_checkpoint_info(filename, device='cpu'):\n",
        "    if os.path.isfile(filename):  # 檢查檔案是否存在\n",
        "        print(f\"=> Loading checkpoint from '{filename}'\")\n",
        "        checkpoint = torch.load(filename, map_location=device, weights_only=False)\n",
        "\n",
        "        # 遍歷 checkpoint 的內容並打印\n",
        "        print(\"Checkpoint contains the following keys and values:\")\n",
        "        for key, value in checkpoint.items():\n",
        "            if isinstance(value, dict):  # 如果值是一個字典，只打印鍵\n",
        "                print(f\"{key}: [Dictionary with keys: {list(value.keys())}]\")\n",
        "            elif isinstance(value, list):  # 如果值是一個列表，只打印長度\n",
        "                print(f\"{key}: [List with length: {len(value)}]\")\n",
        "            elif isinstance(value, torch.Tensor):  # 如果是 Tensor，打印形狀\n",
        "                print(f\"{key}: [Tensor with shape: {value.shape}]\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "    else:\n",
        "        print(f\"=> No checkpoint found at '{filename}'\")\n",
        "\n",
        "print_checkpoint_info('checkpoint.pth', device='cpu')\n",
        "print(\"-----------------\")\n",
        "import torch\n",
        "\n",
        "def print_model_weights(model_path):\n",
        "    if not os.path.isfile(model_path):\n",
        "        print(f\"File '{model_path}' does not exist!\")\n",
        "        return\n",
        "\n",
        "    print(f\"=> Loading model from '{model_path}'\")\n",
        "    state_dict = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "    print(\"Model contains the following layers and weights:\")\n",
        "    for layer_name, weights in state_dict.items():\n",
        "        print(f\"{layer_name}: {weights.shape}\")\n",
        "\n",
        "print_model_weights('results/model_best.pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "STtqWEFF0bZN",
        "outputId": "d9052068-956c-4680-c6a4-d5dd6db2eca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint from 'checkpoint.pth'\n",
            "Checkpoint contains the following keys and values:\n",
            "epoch: 150\n",
            "best_loss: 0.6838059663772583\n",
            "best_epoch: 145\n",
            "best_result: None\n",
            "model_state_dict: [Dictionary with keys: ['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias']]\n",
            "optimizer_state_dict: [Dictionary with keys: ['state', 'param_groups']]\n",
            "-----------------\n",
            "=> Loading model from 'results/model_best.pt'\n",
            "Model contains the following layers and weights:\n",
            "encoder1.enc1conv1.weight: torch.Size([32, 3, 3, 3])\n",
            "encoder1.enc1norm1.weight: torch.Size([32])\n",
            "encoder1.enc1norm1.bias: torch.Size([32])\n",
            "encoder1.enc1norm1.running_mean: torch.Size([32])\n",
            "encoder1.enc1norm1.running_var: torch.Size([32])\n",
            "encoder1.enc1norm1.num_batches_tracked: torch.Size([])\n",
            "encoder1.enc1conv2.weight: torch.Size([32, 32, 3, 3])\n",
            "encoder1.enc1norm2.weight: torch.Size([32])\n",
            "encoder1.enc1norm2.bias: torch.Size([32])\n",
            "encoder1.enc1norm2.running_mean: torch.Size([32])\n",
            "encoder1.enc1norm2.running_var: torch.Size([32])\n",
            "encoder1.enc1norm2.num_batches_tracked: torch.Size([])\n",
            "encoder2.enc2conv1.weight: torch.Size([64, 32, 3, 3])\n",
            "encoder2.enc2norm1.weight: torch.Size([64])\n",
            "encoder2.enc2norm1.bias: torch.Size([64])\n",
            "encoder2.enc2norm1.running_mean: torch.Size([64])\n",
            "encoder2.enc2norm1.running_var: torch.Size([64])\n",
            "encoder2.enc2norm1.num_batches_tracked: torch.Size([])\n",
            "encoder2.enc2conv2.weight: torch.Size([64, 64, 3, 3])\n",
            "encoder2.enc2norm2.weight: torch.Size([64])\n",
            "encoder2.enc2norm2.bias: torch.Size([64])\n",
            "encoder2.enc2norm2.running_mean: torch.Size([64])\n",
            "encoder2.enc2norm2.running_var: torch.Size([64])\n",
            "encoder2.enc2norm2.num_batches_tracked: torch.Size([])\n",
            "encoder3.enc3conv1.weight: torch.Size([128, 64, 3, 3])\n",
            "encoder3.enc3norm1.weight: torch.Size([128])\n",
            "encoder3.enc3norm1.bias: torch.Size([128])\n",
            "encoder3.enc3norm1.running_mean: torch.Size([128])\n",
            "encoder3.enc3norm1.running_var: torch.Size([128])\n",
            "encoder3.enc3norm1.num_batches_tracked: torch.Size([])\n",
            "encoder3.enc3conv2.weight: torch.Size([128, 128, 3, 3])\n",
            "encoder3.enc3norm2.weight: torch.Size([128])\n",
            "encoder3.enc3norm2.bias: torch.Size([128])\n",
            "encoder3.enc3norm2.running_mean: torch.Size([128])\n",
            "encoder3.enc3norm2.running_var: torch.Size([128])\n",
            "encoder3.enc3norm2.num_batches_tracked: torch.Size([])\n",
            "encoder4.enc4conv1.weight: torch.Size([256, 128, 3, 3])\n",
            "encoder4.enc4norm1.weight: torch.Size([256])\n",
            "encoder4.enc4norm1.bias: torch.Size([256])\n",
            "encoder4.enc4norm1.running_mean: torch.Size([256])\n",
            "encoder4.enc4norm1.running_var: torch.Size([256])\n",
            "encoder4.enc4norm1.num_batches_tracked: torch.Size([])\n",
            "encoder4.enc4conv2.weight: torch.Size([256, 256, 3, 3])\n",
            "encoder4.enc4norm2.weight: torch.Size([256])\n",
            "encoder4.enc4norm2.bias: torch.Size([256])\n",
            "encoder4.enc4norm2.running_mean: torch.Size([256])\n",
            "encoder4.enc4norm2.running_var: torch.Size([256])\n",
            "encoder4.enc4norm2.num_batches_tracked: torch.Size([])\n",
            "bottleneck.bottleneckconv1.weight: torch.Size([512, 256, 3, 3])\n",
            "bottleneck.bottlenecknorm1.weight: torch.Size([512])\n",
            "bottleneck.bottlenecknorm1.bias: torch.Size([512])\n",
            "bottleneck.bottlenecknorm1.running_mean: torch.Size([512])\n",
            "bottleneck.bottlenecknorm1.running_var: torch.Size([512])\n",
            "bottleneck.bottlenecknorm1.num_batches_tracked: torch.Size([])\n",
            "bottleneck.bottleneckconv2.weight: torch.Size([512, 512, 3, 3])\n",
            "bottleneck.bottlenecknorm2.weight: torch.Size([512])\n",
            "bottleneck.bottlenecknorm2.bias: torch.Size([512])\n",
            "bottleneck.bottlenecknorm2.running_mean: torch.Size([512])\n",
            "bottleneck.bottlenecknorm2.running_var: torch.Size([512])\n",
            "bottleneck.bottlenecknorm2.num_batches_tracked: torch.Size([])\n",
            "upconv4.weight: torch.Size([512, 256, 2, 2])\n",
            "upconv4.bias: torch.Size([256])\n",
            "decoder4.dec4conv1.weight: torch.Size([256, 512, 3, 3])\n",
            "decoder4.dec4norm1.weight: torch.Size([256])\n",
            "decoder4.dec4norm1.bias: torch.Size([256])\n",
            "decoder4.dec4norm1.running_mean: torch.Size([256])\n",
            "decoder4.dec4norm1.running_var: torch.Size([256])\n",
            "decoder4.dec4norm1.num_batches_tracked: torch.Size([])\n",
            "decoder4.dec4conv2.weight: torch.Size([256, 256, 3, 3])\n",
            "decoder4.dec4norm2.weight: torch.Size([256])\n",
            "decoder4.dec4norm2.bias: torch.Size([256])\n",
            "decoder4.dec4norm2.running_mean: torch.Size([256])\n",
            "decoder4.dec4norm2.running_var: torch.Size([256])\n",
            "decoder4.dec4norm2.num_batches_tracked: torch.Size([])\n",
            "upconv3.weight: torch.Size([256, 128, 2, 2])\n",
            "upconv3.bias: torch.Size([128])\n",
            "decoder3.dec3conv1.weight: torch.Size([128, 256, 3, 3])\n",
            "decoder3.dec3norm1.weight: torch.Size([128])\n",
            "decoder3.dec3norm1.bias: torch.Size([128])\n",
            "decoder3.dec3norm1.running_mean: torch.Size([128])\n",
            "decoder3.dec3norm1.running_var: torch.Size([128])\n",
            "decoder3.dec3norm1.num_batches_tracked: torch.Size([])\n",
            "decoder3.dec3conv2.weight: torch.Size([128, 128, 3, 3])\n",
            "decoder3.dec3norm2.weight: torch.Size([128])\n",
            "decoder3.dec3norm2.bias: torch.Size([128])\n",
            "decoder3.dec3norm2.running_mean: torch.Size([128])\n",
            "decoder3.dec3norm2.running_var: torch.Size([128])\n",
            "decoder3.dec3norm2.num_batches_tracked: torch.Size([])\n",
            "upconv2.weight: torch.Size([128, 64, 2, 2])\n",
            "upconv2.bias: torch.Size([64])\n",
            "decoder2.dec2conv1.weight: torch.Size([64, 128, 3, 3])\n",
            "decoder2.dec2norm1.weight: torch.Size([64])\n",
            "decoder2.dec2norm1.bias: torch.Size([64])\n",
            "decoder2.dec2norm1.running_mean: torch.Size([64])\n",
            "decoder2.dec2norm1.running_var: torch.Size([64])\n",
            "decoder2.dec2norm1.num_batches_tracked: torch.Size([])\n",
            "decoder2.dec2conv2.weight: torch.Size([64, 64, 3, 3])\n",
            "decoder2.dec2norm2.weight: torch.Size([64])\n",
            "decoder2.dec2norm2.bias: torch.Size([64])\n",
            "decoder2.dec2norm2.running_mean: torch.Size([64])\n",
            "decoder2.dec2norm2.running_var: torch.Size([64])\n",
            "decoder2.dec2norm2.num_batches_tracked: torch.Size([])\n",
            "upconv1.weight: torch.Size([64, 32, 2, 2])\n",
            "upconv1.bias: torch.Size([32])\n",
            "decoder1.dec1conv1.weight: torch.Size([32, 64, 3, 3])\n",
            "decoder1.dec1norm1.weight: torch.Size([32])\n",
            "decoder1.dec1norm1.bias: torch.Size([32])\n",
            "decoder1.dec1norm1.running_mean: torch.Size([32])\n",
            "decoder1.dec1norm1.running_var: torch.Size([32])\n",
            "decoder1.dec1norm1.num_batches_tracked: torch.Size([])\n",
            "decoder1.dec1conv2.weight: torch.Size([32, 32, 3, 3])\n",
            "decoder1.dec1norm2.weight: torch.Size([32])\n",
            "decoder1.dec1norm2.bias: torch.Size([32])\n",
            "decoder1.dec1norm2.running_mean: torch.Size([32])\n",
            "decoder1.dec1norm2.running_var: torch.Size([32])\n",
            "decoder1.dec1norm2.num_batches_tracked: torch.Size([])\n",
            "conv.weight: torch.Size([4, 32, 1, 1])\n",
            "conv.bias: torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupTnYetNuNe"
      },
      "source": [
        "Inference: My Dice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QJEAXtUcLSVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "8cbdcbe1-5040-4e91-d4b6-7075c6e171f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckpt:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n",
            "num_classes= 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n",
            "checkpoint:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n",
            "UNet:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for UNet:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([4, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 32, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8a0c4d088715>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 加載 checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 設置 strict=False，允許部分匹配\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;31m#-------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# net.load_state_dict(torch.load(args.ckpt))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([4, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 32, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1])."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "import monai\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.nn.functional import one_hot\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            logits = model(image_batch)\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            loss_dice = dice_loss(logits, label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            # axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "# def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "#     model.eval()\n",
        "#     loss_per_epoch = []\n",
        "#     dice_per_class = np.zeros(args.num_classes)  # save dice per class\n",
        "#     count_per_class = np.zeros(args.num_classes)\n",
        "\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for i_batch, sampled_batch in enumerate(testloader):\n",
        "#             image_batch, label_batch, _ = sampled_batch['image'], sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "#             image_batch, label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long)\n",
        "\n",
        "#             outputs = model(image_batch)\n",
        "#             loss, loss_ce, loss_dice = calc_loss(outputs, label_batch, ce_loss, dice_loss)\n",
        "#             loss_per_epoch.append(loss.item())\n",
        "\n",
        "#             # calaulate dice for each class\n",
        "#             probs = torch.softmax(outputs, dim=1)\n",
        "#             preds = torch.argmax(probs, dim=1)\n",
        "#             for cls in range(args.num_classes):\n",
        "#                 mask_pred = (preds == cls).float()\n",
        "#                 mask_true = (label_batch == cls).float()\n",
        "#                 intersection = (mask_pred * mask_true).sum()\n",
        "#                 union = mask_pred.sum() + mask_true.sum()\n",
        "#                 if union > 0:\n",
        "#                     dice_cls = (2. * intersection) / union\n",
        "#                     dice_per_class[cls] += dice_cls.item()\n",
        "#                     count_per_class[cls] += 1\n",
        "\n",
        "#     # calculate average dice\n",
        "#     avg_dice_per_class = dice_per_class / np.maximum(count_per_class, 1)\n",
        "\n",
        "#     return np.mean(loss_per_epoch), avg_dice_per_class\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    ##################\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/test_dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='test_dataset')\n",
        "    ##################\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'results/model_best.pt'\n",
        "    ckpt = torch.load(args.ckpt)\n",
        "    print(\"ckpt: \",ckpt.keys())  # 查看權重包含哪些鍵值\n",
        "\n",
        "    print(\"num_classes=\", args.num_classes)\n",
        "    net = monai.networks.nets.UNet(\n",
        "        spatial_dims=2,\n",
        "        in_channels=3,\n",
        "        out_channels=args.num_classes + 1,\n",
        "\n",
        "        channels=(16, 32, 64, 128, 256),\n",
        "        strides=(2, 2, 2, 2),\n",
        "        num_res_units=2,\n",
        "    ).to(device)\n",
        "\n",
        "    net = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "                         in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "    print(\"net: \",net.state_dict().keys())  # 檢查目前模型的層名稱\n",
        "\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "#------------------- Check the layer of checkpoint and UNet\n",
        "    checkpoint = torch.load(args.ckpt)\n",
        "    print(\"checkpoint: \",checkpoint.keys())  # 檢查存在哪些 keys\n",
        "\n",
        "\n",
        "    print(\"UNet: \",net.state_dict().keys())  # 列出模型的 keys\n",
        "\n",
        "\n",
        "    state_dict = torch.load(args.ckpt, map_location=\"cpu\")  # 加載 checkpoint\n",
        "    net.load_state_dict(state_dict, strict=False)  # 設置 strict=False，允許部分匹配\n",
        "#-------------------\n",
        "    # net.load_state_dict(torch.load(args.ckpt))\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    testloader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=4)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            logits = model(image_batch)\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            loss_dice = dice_loss(logits, label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            print(\"Image batch shape:\", image_batch.shape)\n",
        "            # axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    ##################\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/test_dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='test_dataset')\n",
        "    ##################\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'results/model_best.pt'\n",
        "    net = monai.networks.nets.UNet(\n",
        "        spatial_dims=2,\n",
        "        in_channels=3,\n",
        "        out_channels=args.num_classes + 1,\n",
        "        channels=(16, 32, 64, 128, 256),\n",
        "        strides=(2, 2, 2, 2),\n",
        "        num_res_units=2,\n",
        "    ).to(device)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "#------------------- Check the layer of checkpoint and UNet\n",
        "    checkpoint = torch.load(args.ckpt)\n",
        "    print(\"checkpoint: \",checkpoint.keys())  # 檢查存在哪些 keys\n",
        "\n",
        "\n",
        "    print(\"UNet: \",net.state_dict().keys())  # 列出模型的 keys\n",
        "\n",
        "\n",
        "    state_dict = torch.load(args.ckpt, map_location=\"cpu\")  # 加載 checkpoint\n",
        "    net.load_state_dict(state_dict, strict=False)  # 設置 strict=False，允許部分匹配\n",
        "#-------------------\n",
        "    # net.load_state_dict(torch.load(args.ckpt))\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ],
      "metadata": {
        "id": "ItcyRlMK63wK",
        "outputId": "fa45a709-ca7f-449e-86b6-041def59c3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n",
            "UNet:  odict_keys(['model.0.conv.unit0.conv.weight', 'model.0.conv.unit0.conv.bias', 'model.0.conv.unit0.adn.A.weight', 'model.0.conv.unit1.conv.weight', 'model.0.conv.unit1.conv.bias', 'model.0.conv.unit1.adn.A.weight', 'model.0.residual.weight', 'model.0.residual.bias', 'model.1.submodule.0.conv.unit0.conv.weight', 'model.1.submodule.0.conv.unit0.conv.bias', 'model.1.submodule.0.conv.unit0.adn.A.weight', 'model.1.submodule.0.conv.unit1.conv.weight', 'model.1.submodule.0.conv.unit1.conv.bias', 'model.1.submodule.0.conv.unit1.adn.A.weight', 'model.1.submodule.0.residual.weight', 'model.1.submodule.0.residual.bias', 'model.1.submodule.1.submodule.0.conv.unit0.conv.weight', 'model.1.submodule.1.submodule.0.conv.unit0.conv.bias', 'model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight', 'model.1.submodule.1.submodule.0.conv.unit1.conv.weight', 'model.1.submodule.1.submodule.0.conv.unit1.conv.bias', 'model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight', 'model.1.submodule.1.submodule.0.residual.weight', 'model.1.submodule.1.submodule.0.residual.bias', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.weight', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.bias', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit0.adn.A.weight', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.weight', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.bias', 'model.1.submodule.1.submodule.1.submodule.0.conv.unit1.adn.A.weight', 'model.1.submodule.1.submodule.1.submodule.0.residual.weight', 'model.1.submodule.1.submodule.1.submodule.0.residual.bias', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias', 'model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight', 'model.1.submodule.1.submodule.1.submodule.1.submodule.residual.weight', 'model.1.submodule.1.submodule.1.submodule.1.submodule.residual.bias', 'model.1.submodule.1.submodule.1.submodule.2.0.conv.weight', 'model.1.submodule.1.submodule.1.submodule.2.0.conv.bias', 'model.1.submodule.1.submodule.1.submodule.2.0.adn.A.weight', 'model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.weight', 'model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.bias', 'model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight', 'model.1.submodule.1.submodule.2.0.conv.weight', 'model.1.submodule.1.submodule.2.0.conv.bias', 'model.1.submodule.1.submodule.2.0.adn.A.weight', 'model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight', 'model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias', 'model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight', 'model.1.submodule.2.0.conv.weight', 'model.1.submodule.2.0.conv.bias', 'model.1.submodule.2.0.adn.A.weight', 'model.1.submodule.2.1.conv.unit0.conv.weight', 'model.1.submodule.2.1.conv.unit0.conv.bias', 'model.1.submodule.2.1.conv.unit0.adn.A.weight', 'model.2.0.conv.weight', 'model.2.0.conv.bias', 'model.2.0.adn.A.weight', 'model.2.1.conv.unit0.conv.weight', 'model.2.1.conv.unit0.conv.bias'])\n",
            "Image batch shape: torch.Size([12, 3, 512, 512])\n",
            "Class Wise Dice: {'dice_cls:1': 0.2853732700559834, 'dice_cls:2': 0.4221294438721105, 'dice_cls:3': 0.16652994257588188}\n",
            "Overall Dice: 0.2913442188346586\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x100 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAABhCAYAAADyU8z6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAhpJREFUeJzt2MFqo1AYhuHf0K12L3j/FyZ4Abr3dJVhUgZ6ytiZNt/zgDsDhw9eYzK01loBEW7/+wDAvyN4CCJ4CCJ4CCJ4CCJ4CCJ4CCJ4CPLSc9N5nrVtW43jWMMwfPWZvr3WWh3HUfM81+3W/8y04yM7Xqd7y9ZhXddWVa5317quPfPZ0Y7fZsuub/hxHKuqal3Xmqap5yNPbd/3Wpbl1y697PjIjtfp3bIr+Ptr0zRNBv7NZ18n7fhndrzOR1v60w6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CCB6CvPTc1Fqrqqp937/0MD/FfYf7Lr3s+MiO1+ndsiv44ziqqmpZlr881nM5jqNeX18/dX+VHd+z43U+2nJoHY/X8zxr27Yax7GGYbj0gD9Ra62O46h5nut26/9VZMdHdrxO75ZdwQPPwZ92EETwEETwEETwEETwEETwEETwEOQNoFca7nGh39sAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import monai\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            logits = model(image_batch)\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            loss_dice = dice_loss(logits, label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            # axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            # axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    ##################\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/L3D_Dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='L3D_Dataset')\n",
        "    ##################\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=3)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'results/model_best.pt'\n",
        "    ckpt = torch.load(args.ckpt)\n",
        "    print(\"ckpt: \",ckpt.keys())  # 查看權重包含哪些鍵值\n",
        "\n",
        "    print(\"num_classes=\", args.num_classes)\n",
        "    net = monai.networks.nets.UNet(\n",
        "        spatial_dims=2,\n",
        "        in_channels=3,\n",
        "        out_channels=args.num_classes + 1,\n",
        "\n",
        "        channels=(16, 32, 64, 128, 256),\n",
        "        strides=(2, 2, 2, 2),\n",
        "        num_res_units=2,\n",
        "    ).to(device)\n",
        "\n",
        "    net = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "                         in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "    print(\"net: \",net.state_dict().keys())  # 檢查目前模型的層名稱\n",
        "\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    net.load_state_dict(torch.load(args.ckpt))\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    testloader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ],
      "metadata": {
        "id": "Cs5fbrPjxKrI",
        "outputId": "dce617ce-31c2-441d-fb70-1517a6f3ab71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckpt:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n",
            "num_classes= 3\n",
            "net:  odict_keys(['encoder1.enc1conv1.weight', 'encoder1.enc1norm1.weight', 'encoder1.enc1norm1.bias', 'encoder1.enc1norm1.running_mean', 'encoder1.enc1norm1.running_var', 'encoder1.enc1norm1.num_batches_tracked', 'encoder1.enc1conv2.weight', 'encoder1.enc1norm2.weight', 'encoder1.enc1norm2.bias', 'encoder1.enc1norm2.running_mean', 'encoder1.enc1norm2.running_var', 'encoder1.enc1norm2.num_batches_tracked', 'encoder2.enc2conv1.weight', 'encoder2.enc2norm1.weight', 'encoder2.enc2norm1.bias', 'encoder2.enc2norm1.running_mean', 'encoder2.enc2norm1.running_var', 'encoder2.enc2norm1.num_batches_tracked', 'encoder2.enc2conv2.weight', 'encoder2.enc2norm2.weight', 'encoder2.enc2norm2.bias', 'encoder2.enc2norm2.running_mean', 'encoder2.enc2norm2.running_var', 'encoder2.enc2norm2.num_batches_tracked', 'encoder3.enc3conv1.weight', 'encoder3.enc3norm1.weight', 'encoder3.enc3norm1.bias', 'encoder3.enc3norm1.running_mean', 'encoder3.enc3norm1.running_var', 'encoder3.enc3norm1.num_batches_tracked', 'encoder3.enc3conv2.weight', 'encoder3.enc3norm2.weight', 'encoder3.enc3norm2.bias', 'encoder3.enc3norm2.running_mean', 'encoder3.enc3norm2.running_var', 'encoder3.enc3norm2.num_batches_tracked', 'encoder4.enc4conv1.weight', 'encoder4.enc4norm1.weight', 'encoder4.enc4norm1.bias', 'encoder4.enc4norm1.running_mean', 'encoder4.enc4norm1.running_var', 'encoder4.enc4norm1.num_batches_tracked', 'encoder4.enc4conv2.weight', 'encoder4.enc4norm2.weight', 'encoder4.enc4norm2.bias', 'encoder4.enc4norm2.running_mean', 'encoder4.enc4norm2.running_var', 'encoder4.enc4norm2.num_batches_tracked', 'bottleneck.bottleneckconv1.weight', 'bottleneck.bottlenecknorm1.weight', 'bottleneck.bottlenecknorm1.bias', 'bottleneck.bottlenecknorm1.running_mean', 'bottleneck.bottlenecknorm1.running_var', 'bottleneck.bottlenecknorm1.num_batches_tracked', 'bottleneck.bottleneckconv2.weight', 'bottleneck.bottlenecknorm2.weight', 'bottleneck.bottlenecknorm2.bias', 'bottleneck.bottlenecknorm2.running_mean', 'bottleneck.bottlenecknorm2.running_var', 'bottleneck.bottlenecknorm2.num_batches_tracked', 'upconv4.weight', 'upconv4.bias', 'decoder4.dec4conv1.weight', 'decoder4.dec4norm1.weight', 'decoder4.dec4norm1.bias', 'decoder4.dec4norm1.running_mean', 'decoder4.dec4norm1.running_var', 'decoder4.dec4norm1.num_batches_tracked', 'decoder4.dec4conv2.weight', 'decoder4.dec4norm2.weight', 'decoder4.dec4norm2.bias', 'decoder4.dec4norm2.running_mean', 'decoder4.dec4norm2.running_var', 'decoder4.dec4norm2.num_batches_tracked', 'upconv3.weight', 'upconv3.bias', 'decoder3.dec3conv1.weight', 'decoder3.dec3norm1.weight', 'decoder3.dec3norm1.bias', 'decoder3.dec3norm1.running_mean', 'decoder3.dec3norm1.running_var', 'decoder3.dec3norm1.num_batches_tracked', 'decoder3.dec3conv2.weight', 'decoder3.dec3norm2.weight', 'decoder3.dec3norm2.bias', 'decoder3.dec3norm2.running_mean', 'decoder3.dec3norm2.running_var', 'decoder3.dec3norm2.num_batches_tracked', 'upconv2.weight', 'upconv2.bias', 'decoder2.dec2conv1.weight', 'decoder2.dec2norm1.weight', 'decoder2.dec2norm1.bias', 'decoder2.dec2norm1.running_mean', 'decoder2.dec2norm1.running_var', 'decoder2.dec2norm1.num_batches_tracked', 'decoder2.dec2conv2.weight', 'decoder2.dec2norm2.weight', 'decoder2.dec2norm2.bias', 'decoder2.dec2norm2.running_mean', 'decoder2.dec2norm2.running_var', 'decoder2.dec2norm2.num_batches_tracked', 'upconv1.weight', 'upconv1.bias', 'decoder1.dec1conv1.weight', 'decoder1.dec1norm1.weight', 'decoder1.dec1norm1.bias', 'decoder1.dec1norm1.running_mean', 'decoder1.dec1norm1.running_var', 'decoder1.dec1norm1.num_batches_tracked', 'decoder1.dec1conv2.weight', 'decoder1.dec1norm2.weight', 'decoder1.dec1norm2.bias', 'decoder1.dec1norm2.running_mean', 'decoder1.dec1norm2.running_var', 'decoder1.dec1norm2.num_batches_tracked', 'conv.weight', 'conv.bias'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for UNet:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([4, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 32, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-cbd7927096aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdice_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegmentationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([4, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 32, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AR3WsfSk19--"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}