{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/SAM-Liver/blob/main/SAMed_Liver_Contour_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SAMed\n",
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics\n",
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "huO1simPGzHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487e4469-8e29-4f93-89be-fe2fb58cad87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/151.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 2.61 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# this is the small dataset based on L3D_Dataset, change to L3D_Dataset when the code is ready\n",
        "# !gdown --fuzzy https://drive.google.com/file/d/18KVLU4y0BPoRqtHnhsE3KbTQb2BEUXTz/view?usp=sharing\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1/view?usp=drive_link  #L3D_dataset\n",
        "!gdown 1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
        "!gdown 1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
        "!ls"
      ],
      "metadata": {
        "id": "5lxkGZBB9-FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa924b7e-988b-4472-e440-e3d1960c5227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JyTYDCkv3RGUnkE--l-Kf9v5PLOCI3k1\n",
            "To: /content/samed_codes/L3D_Dataset.zip\n",
            "100% 3.64G/3.64G [00:24<00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
            "To: /content/samed_codes/epoch_159.pth\n",
            "100% 19.7M/19.7M [00:00<00:00, 40.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
            "To: /content/samed_codes/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:01<00:00, 208MB/s]\n",
            "datasets\t materials\t   sam_lora_image_encoder_mask_decoder.py  test.py\n",
            "epoch_159.pth\t preprocess\t   sam_lora_image_encoder.py\t\t   trainer.py\n",
            "L3D_Dataset.zip  README.md\t   sam_vit_b_01ec64.pth\t\t\t   train.py\n",
            "LICENSE\t\t requirements.txt  segment_anything\t\t\t   utils.py\n",
            "lists\t\t SAMed_h\t   subsample_datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q L3D_Dataset.zip"
      ],
      "metadata": {
        "id": "3XwGX62q92V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtTdD6SGxBc7",
        "outputId": "d6b3df87-928f-4c32-b79a-233f01734036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/samed_codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2 as iio\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import zoom\n",
        "from glob import glob\n",
        "from einops import repeat\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root='/content/samed_codes/test_dataset/Train', low_res=None, transform_img=None, transform_mask=None, istrain=False):\n",
        "        self.img_path_all = glob(root + '/images/*.jpg')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/masks_gt/*.png')  # Update the path and pattern\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "        self.istrain = istrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "        self.brightness = 0.1\n",
        "        self.contrast = 0.1\n",
        "        self.saturation = 0.1\n",
        "        self.hue = 0.1\n",
        "        self.color_aug = transforms.ColorJitter(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_path_all[idx]\n",
        "        mask_path = self.mask_path_all[idx]\n",
        "        # print(f\"Found {len(self.img_path_all)} images and {len(self.mask_path_all)} masks\")\n",
        "\n",
        "        # Open image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L') #29(3), 76(1), 150 (2)\n",
        "        mask = np.array(mask)\n",
        "        # class mapping\n",
        "        mask[mask==0] = 0   #background\n",
        "        mask[mask==76] = 1  #red\n",
        "        mask[mask==150] = 2 #green\n",
        "        mask[mask==29] = 3 #blue\n",
        "        mask = Image.fromarray(mask)\n",
        "        # Apply transformations if provided\n",
        "        if self.istrain:\n",
        "            hflip = random.random() < 0.5\n",
        "            flip_container = random.choice([Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM])\n",
        "            if hflip:\n",
        "                # image = self.color_aug(image)\n",
        "                image = image.transpose(flip_container)\n",
        "                mask = mask.transpose(flip_container)\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)  # Convert first channel of mask to Image format\n",
        "        # image = transforms.ToTensor()(image)  # **Converting image to tensor**\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        sample = {'image': image, 'mask': mask}\n",
        "\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(mask, (self.low_res/mask.shape[0], self.low_res/mask.shape[1]), order=0)\n",
        "            sample = {'image': image, 'mask': mask, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "    # transforms.ToTensor(),\n",
        "    ])\n",
        "ds = SegmentationDataset(transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "for sample in ds:\n",
        "    print(np.unique(sample['mask']))  # Check unique values in the mask (class IDs)\n",
        "    plt.subplot(121); plt.imshow(sample['image'].permute(1, 2, 0))  # Rearrange image channels for display\n",
        "    plt.subplot(122); plt.imshow(sample['mask'])  # Display the mask\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "s6BpXai3-SWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9f880b-3887-436d-9176-598e200770c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA"
      ],
      "metadata": {
        "id": "IV6KyKtRIyBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from segment_anything.modeling import Sam\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "class _LoRA_qkv_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            qkv: nn.Module,\n",
        "            linear_a_q: nn.Module,\n",
        "            linear_b_q: nn.Module,\n",
        "            linear_a_v: nn.Module,\n",
        "            linear_b_v: nn.Module,\n",
        "            conv_se_q: nn.Module,\n",
        "            conv_se_v: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = qkv\n",
        "        self.linear_a_q = linear_a_q\n",
        "        self.linear_b_q = linear_b_q\n",
        "        self.linear_a_v = linear_a_v\n",
        "        self.linear_b_v = linear_b_v\n",
        "        self.conv_se_q = conv_se_q\n",
        "        self.conv_se_v = conv_se_v\n",
        "\n",
        "        self.dim = qkv.in_features\n",
        "        self.w_identity = torch.eye(qkv.in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv(x)\n",
        "        a_q_out = self.linear_a_q(x)\n",
        "        a_v_out = self.linear_a_v(x)\n",
        "        a_q_out_temp = self.conv_se_q(a_q_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "        a_v_out_temp = self.conv_se_v(a_v_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "\n",
        "        new_q = self.linear_b_q(torch.mul(a_q_out, torch.sigmoid(a_q_out_temp)))#SE = Squeeze and Excitation\n",
        "        new_v = self.linear_b_v(torch.mul(a_v_out, torch.sigmoid(a_v_out_temp)))\n",
        "\n",
        "        qkv[:, :, :, : self.dim] += new_q\n",
        "        qkv[:, :, :, -self.dim:] += new_v\n",
        "        return qkv\n",
        "\n",
        "class LoRA_Sam_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
        "        super(LoRA_Sam_v0_v2, self).__init__()\n",
        "\n",
        "        assert r > 0\n",
        "        if lora_layer:\n",
        "            self.lora_layer = lora_layer\n",
        "        else:\n",
        "            self.lora_layer = list(\n",
        "                range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
        "        # create for storage, then we can init them or load weights\n",
        "        self.w_As = []  # These are linear layers\n",
        "        self.w_Bs = []\n",
        "\n",
        "        # lets freeze first\n",
        "        for param in sam_model.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Here, we do the surgery\n",
        "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
        "            # If we only want few lora layer instead of all\n",
        "            if t_layer_i not in self.lora_layer:\n",
        "                continue\n",
        "            w_qkv_linear = blk.attn.qkv\n",
        "            self.dim = w_qkv_linear.in_features\n",
        "            w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
        "            w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
        "\n",
        "            conv_se_q = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            conv_se_v = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            self.w_As.append(w_a_linear_q)\n",
        "            self.w_Bs.append(w_b_linear_q)\n",
        "            self.w_As.append(w_a_linear_v)\n",
        "            self.w_Bs.append(w_b_linear_v)\n",
        "            self.w_As.append(conv_se_q)\n",
        "            self.w_As.append(conv_se_v)\n",
        "            blk.attn.qkv = _LoRA_qkv_v0_v2(\n",
        "                w_qkv_linear,\n",
        "                w_a_linear_q,\n",
        "                w_b_linear_q,\n",
        "                w_a_linear_v,\n",
        "                w_b_linear_v,\n",
        "                conv_se_q,\n",
        "                conv_se_v,\n",
        "            )\n",
        "        self.reset_parameters()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def save_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\n",
        "\n",
        "        save both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        num_layer = len(self.w_As)  # actually, it is half\n",
        "        a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
        "        num_layer = len(self.w_Bs)  # actually, it is half\n",
        "        b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
        "        prompt_encoder_tensors = {}\n",
        "        mask_decoder_tensors = {}\n",
        "\n",
        "        # save prompt encoder, only `state_dict`, the `named_parameter` is not permitted\n",
        "        if isinstance(self.sam, torch.nn.DataParallel) or isinstance(self.sam, torch.nn.parallel.DistributedDataParallel):\n",
        "            state_dict = self.sam.module.state_dict()\n",
        "        else:\n",
        "            state_dict = self.sam.state_dict()\n",
        "        for key, value in state_dict.items():\n",
        "            if 'prompt_encoder' in key:\n",
        "                prompt_encoder_tensors[key] = value\n",
        "            if 'mask_decoder' in key:\n",
        "                mask_decoder_tensors[key] = value\n",
        "\n",
        "        merged_dict = {**a_tensors, **b_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}\n",
        "        torch.save(merged_dict, filename)\n",
        "\n",
        "    def load_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\\\n",
        "\n",
        "        load both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        state_dict = torch.load(filename)\n",
        "\n",
        "        for i, w_A_linear in enumerate(self.w_As):\n",
        "            saved_key = f\"w_a_{i:03d}\"\n",
        "            # print('mobarak:', saved_key)\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_A_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        for i, w_B_linear in enumerate(self.w_Bs):\n",
        "            saved_key = f\"w_b_{i:03d}\"\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_B_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        sam_dict = self.sam.state_dict()\n",
        "        sam_keys = sam_dict.keys()\n",
        "\n",
        "        # load prompt encoder\n",
        "        prompt_encoder_keys = [k for k in sam_keys if 'prompt_encoder' in k]\n",
        "        prompt_encoder_values = [state_dict[k] for k in prompt_encoder_keys]\n",
        "        prompt_encoder_new_state_dict = {k: v for k, v in zip(prompt_encoder_keys, prompt_encoder_values)}\n",
        "        sam_dict.update(prompt_encoder_new_state_dict)\n",
        "\n",
        "        # load mask decoder\n",
        "        mask_decoder_keys = [k for k in sam_keys if 'mask_decoder' in k]\n",
        "        mask_decoder_values = [state_dict[k] for k in mask_decoder_keys]\n",
        "        mask_decoder_new_state_dict = {k: v for k, v in zip(mask_decoder_keys, mask_decoder_values)}\n",
        "        sam_dict.update(mask_decoder_new_state_dict)\n",
        "        self.sam.load_state_dict(sam_dict)\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w_A in self.w_As:\n",
        "            nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
        "        for w_B in self.w_Bs:\n",
        "            nn.init.zeros_(w_B.weight)\n",
        "\n",
        "    def forward(self, batched_input, multimask_output, image_size):\n",
        "        return self.sam(batched_input, multimask_output, image_size)"
      ],
      "metadata": {
        "id": "E8zBUeaO8JtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c465560-e1a9-44f2-bc32-60ede1abe33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize & Save Check point"
      ],
      "metadata": {
        "id": "tjKECv_L9plE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_inference(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()  # Disable dropout and batch normalization\n",
        "    num_samples_to_plot = 5  # Number of images to visualize\n",
        "\n",
        "    fig, axs = plt.subplots(num_samples_to_plot, 3, figsize=(12, num_samples_to_plot * 4),\n",
        "                            subplot_kw=dict(xticks=[], yticks=[]))\n",
        "    fig.suptitle(\"Input Image | Predicted Mask | Ground Truth\", fontsize=16)\n",
        "\n",
        "    num_classes = args.num_classes + 1  # Include extra class\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            # Break early if we have plotted enough samples\n",
        "            if i_batch >= num_samples_to_plot:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                image_batch, label_batch, low_res_label_batch = (\n",
        "                    sampled_batch['image'],\n",
        "                    sampled_batch['mask'],\n",
        "                    sampled_batch['low_res_label']\n",
        "                )\n",
        "                image_batch = image_batch.to(device, dtype=torch.float32)\n",
        "                label_batch = label_batch.to(device, dtype=torch.long)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading data for batch {i_batch + 1}: {e}\")\n",
        "                continue  # Skip this batch\n",
        "\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "\n",
        "            # Convert to NumPy arrays for visualization\n",
        "            input_image = image_batch[0].cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
        "            pred_mask = pred_seg[0].cpu().numpy()  # Predicted segmentation\n",
        "            true_mask = label_batch[0].cpu().numpy()  # Ground truth\n",
        "\n",
        "            # Normalize image if needed\n",
        "            input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
        "\n",
        "            # Plot the images\n",
        "            axs[i_batch, 0].imshow(input_image)\n",
        "            axs[i_batch, 0].set_title(\"Input Image\")\n",
        "\n",
        "            axs[i_batch, 1].imshow(pred_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 1].set_title(\"Predicted Mask\")\n",
        "\n",
        "            axs[i_batch, 2].imshow(true_mask, cmap=\"jet\", interpolation=\"none\")\n",
        "            axs[i_batch, 2].set_title(\"Ground Truth Mask\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, epoch_since_improvement, best_result, checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'best_loss': best_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        'epoch_since_improvement': epoch_since_improvement,  # 儲存 epoch_since_improvement\n",
        "        'best_result': best_result,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} with loss {best_loss} and best epoch {best_epoch}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "  if os.path.isfile(filename):\n",
        "    print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "    checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    best_loss = checkpoint.get('best_loss', np.inf)\n",
        "    best_epoch = checkpoint.get('best_epoch', 0)\n",
        "    epoch_since_improvement = checkpoint.get('epoch_since_improvement', 0)\n",
        "    best_result = checkpoint.get('best_result', None)\n",
        "\n",
        "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint.get('epoch', 'N/A')))  # load epoch\n",
        "\n",
        "    return start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result\n",
        "\n",
        "  else:\n",
        "    print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "    return 1, [0, 0], 0, 0, None"
      ],
      "metadata": {
        "id": "xR2tnwt49sr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Dataloader & Check unique pixels"
      ],
      "metadata": {
        "id": "suWo_5_gjIzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_dataloader(dataloader, num_classes=4):\n",
        "    \"\"\"\n",
        "    檢查 DataLoader 是否正確地處理了影像和 mask。\n",
        "\n",
        "    Args:\n",
        "        dataloader: PyTorch 的 DataLoader\n",
        "        num_classes: 預期的類別數（例如，2 表示二分類，N 表示多分類）\n",
        "    \"\"\"\n",
        "    for idx, batch in enumerate(dataloader):  # 迴圈讀取每個 batch\n",
        "        # 根據 batch 的結構提取 image 和 mask\n",
        "        image = batch[\"image\"]\n",
        "        mask = batch[\"mask\"]\n",
        "        low_res_label = batch[\"low_res_label\"]\n",
        "\n",
        "        print(f\"Batch {idx + 1}:\")\n",
        "        print(f\"Image shape: {image.shape}\")\n",
        "        print(f\"Mask shape: {mask.shape}\")\n",
        "        # print(f\"Low-res label shape: {low_res_label.shape}\")\n",
        "\n",
        "        unique_values = torch.unique(mask)   # find unique values (class) of the images, check if there are 4 class (0, 1, 2, 3)\n",
        "        print(f\"  Unique values in mask: {unique_values.tolist()}\")\n",
        "\n",
        "        if unique_values.max() >= num_classes:\n",
        "            print(f\"!!!! Mask contains values >= {num_classes}.\")\n",
        "        if unique_values.min() < 0:\n",
        "            print(f\"!!!! Mask contains negative values.\")\n",
        "\n",
        "        # 視覺化部分影像及其 mask（每個 batch 中只視覺化前 2 張）\n",
        "        for i in range(min(2, image.size(0))):  # image.size(0) 是 batch 中的樣本數\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(image[i].permute(1, 2, 0).cpu().numpy())\n",
        "            plt.title(f\"Batch {idx + 1} - Image {i + 1}\")\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(mask[i].cpu().numpy(), cmap=\"gray\")\n",
        "            plt.title(f\"Batch {idx + 1} - Mask {i + 1}\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    print(\"Finished verifying all batches in the dataloader.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TRUXcEedjOnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM Training"
      ],
      "metadata": {
        "id": "2XLzKt8YaiAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adjust_learning_rate(optimizer, iter_num, args):\n",
        "    if args.warmup and iter_num < args.warmup_period:\n",
        "        lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "    else:\n",
        "        if args.warmup:\n",
        "            shift_iter = iter_num - args.warmup_period\n",
        "            assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "        else:\n",
        "            shift_iter = iter_num\n",
        "        lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "    return lr_\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def inference_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    # fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(output, label_batch, ce_loss, dice_loss, args):\n",
        "    # print(\"label_batch\",label_batch.shape)\n",
        "    # print(output.shape)\n",
        "    loss_ce = ce_loss(output, label_batch[:].long())\n",
        "    loss_dice = dice_loss(output, label_batch, softmax=True)\n",
        "    loss = (1 - args.dice_weight) * loss_ce + args.dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "        outputs = model(image_batch, multimask_output, args.img_size)\n",
        "        output = outputs[args.output_key]\n",
        "        loss_label_batch = batch_dict[args.batch_key]\n",
        "        loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update learning rate and increment iteration count\n",
        "        lr_current = adjust_learning_rate(optimizer, iter_num, args)\n",
        "        iter_num += 1\n",
        "\n",
        "        loss_all.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(loss_all), iter_num, lr_current\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['mask'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            batch_dict = {'image_batch':image_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            output = outputs[args.output_key]\n",
        "            loss_label_batch = batch_dict[args.batch_key]\n",
        "            loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Add new arguments\n",
        "    parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "    parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "    parser.add_argument('--weights', type=int, nargs='+', default=None,help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='/content/samed_codes/L3D_Dataset')\n",
        "    parser.add_argument('--data_path', type=str, default='L3D_Dataset')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=4)  ###########\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--output_file', type=str, default='model_best.pt') ############\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int, default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=60, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/' + args.output_file\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size, num_classes=args.num_classes,\n",
        "                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0], pixel_std=[1, 1, 1])\n",
        "\n",
        "    # pkg = import_module(args.module)\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    # net.load_lora_parameters(args.lora_ckpt)\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "\n",
        "    transform_img = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    transform_mask = transforms.Compose([\n",
        "        transforms.Resize((512, 512), interpolation=InterpolationMode.NEAREST),\n",
        "        # transforms.ToTensor(),\n",
        "        ])\n",
        "# ds = SegmentationDataset(transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "\n",
        "    train_dataset = SegmentationDataset(root=(args.data_path+'/Train'), low_res=128, transform_img=transform_img, transform_mask=transform_mask, istrain=True)\n",
        "    test_dataset = SegmentationDataset(root=(args.data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    valid_dataset = SegmentationDataset(root=(args.data_path+'/Val'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    # #####\n",
        "    # for idx, batch in enumerate(trainloader):\n",
        "    #   print(f\"Batch {idx + 1}: {type(batch)}\")\n",
        "    #   # print(f\"Batch content: {batch}\")\n",
        "    #   break  # 只檢查第一個 batch\n",
        "    # print(\"Verifying train_loader...\")\n",
        "    # verify_dataloader(trainloader, 4)\n",
        "    # print(\"Verifying valid_loader...\")\n",
        "    # verify_dataloader(valid_loader, 4)\n",
        "    # print(\"Verifying test_loader...\")\n",
        "    # verify_dataloader(testloader, 4)\n",
        "    # #####\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    # # test if there is low_res_label\n",
        "    # for i_batch, sampled_batch in enumerate(trainloader):\n",
        "    #   print(f\"Sampled batch keys: {sampled_batch.keys()}\")  # 確認是否有 'low_res_label'\n",
        "\n",
        "\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "    ######### load checkpoint\n",
        "    start_epoch, best_loss, best_epoch, epoch_since_improvement, best_result = load_checkpoint(net, optimizer, 'checkpoint.pth')\n",
        "\n",
        "    if start_epoch == 1 and isinstance(best_loss, list) and best_loss == [0, 0]:\n",
        "        print(\"No checkpoint loaded; starting training from scratch.\")\n",
        "        best_loss = np.inf\n",
        "    elif start_epoch == args.max_epochs:\n",
        "        print(\"Start training from scratch.\")\n",
        "        start_epoch = 0\n",
        "        best_loss = np.inf\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} with best loss {best_loss}, best epoch {best_epoch}.\")\n",
        "    ########\n",
        "\n",
        "    for epoch in range(start_epoch, args.max_epochs):\n",
        "        loss_training, iter_num, lr_current = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        ##### save checkpoint\n",
        "        checkpoint_path = '/content/samed_codes/checkpoint.pth'\n",
        "        save_checkpoint(net, optimizer, epoch, best_loss, best_epoch, epoch_since_improvement, best_result, checkpoint_path)\n",
        "        #####\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            net.save_lora_parameters(os.path.join(args.output_dir, args.output_file))\n",
        "\n",
        "        print('--- Epoch {}/{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "    format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, lr_current))\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = inference_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))\n",
        "    # visualize\n",
        "    plot_inference(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    #####\n",
        "    test = verify_dataloader(testloader, num_classes=4)\n",
        "    print(\"verify_test\", test)\n",
        "    train = verify_dataloader(trainloader, num_classes=4)\n",
        "    print(\"verify_train\", train)\n",
        "\n",
        "    #####\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # safe checkpoint\n",
        "    checkpoint_path = 'checkpoints'\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    main()"
      ],
      "metadata": {
        "id": "36CLN8Rj7MgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed87c828-bade-468b-9e5f-cd0c334f09f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Training on: cuda train sample size: 921 test sample size: 109 batch: 8\n",
            "=> no checkpoint found at 'checkpoint.pth'\n",
            "No checkpoint loaded; starting training from scratch.\n",
            "Checkpoint saved at epoch 1 with loss inf and best epoch 0\n",
            "--- Epoch 1/100: Training loss = 0.6482, Testing: [loss = 0.4904, dice = 0.4537], Best loss = 0.4904, Best epoch = 1, lr = 0.002320\n",
            "Checkpoint saved at epoch 2 with loss 0.4904283157416752 and best epoch 1\n",
            "--- Epoch 2/100: Training loss = 0.4911, Testing: [loss = 0.4987, dice = 0.4750], Best loss = 0.4904, Best epoch = 1, lr = 0.004640\n",
            "Checkpoint saved at epoch 3 with loss 0.4904283157416752 and best epoch 1\n",
            "--- Epoch 3/100: Training loss = 0.4855, Testing: [loss = 0.4806, dice = 0.4710], Best loss = 0.4806, Best epoch = 3, lr = 0.004985\n",
            "Checkpoint saved at epoch 4 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 4/100: Training loss = 0.4841, Testing: [loss = 0.4828, dice = 0.4688], Best loss = 0.4806, Best epoch = 3, lr = 0.004968\n",
            "Checkpoint saved at epoch 5 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 5/100: Training loss = 0.4823, Testing: [loss = 0.4857, dice = 0.4691], Best loss = 0.4806, Best epoch = 3, lr = 0.004951\n",
            "Checkpoint saved at epoch 6 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 6/100: Training loss = 0.4817, Testing: [loss = 0.4818, dice = 0.4696], Best loss = 0.4806, Best epoch = 3, lr = 0.004933\n",
            "Checkpoint saved at epoch 7 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 7/100: Training loss = 0.4813, Testing: [loss = 0.4820, dice = 0.4706], Best loss = 0.4806, Best epoch = 3, lr = 0.004916\n",
            "Checkpoint saved at epoch 8 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 8/100: Training loss = 0.4806, Testing: [loss = 0.4807, dice = 0.4655], Best loss = 0.4806, Best epoch = 3, lr = 0.004898\n",
            "Checkpoint saved at epoch 9 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 9/100: Training loss = 0.4813, Testing: [loss = 0.4816, dice = 0.4643], Best loss = 0.4806, Best epoch = 3, lr = 0.004881\n",
            "Checkpoint saved at epoch 10 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 10/100: Training loss = 0.4808, Testing: [loss = 0.4889, dice = 0.4509], Best loss = 0.4806, Best epoch = 3, lr = 0.004863\n",
            "Checkpoint saved at epoch 11 with loss 0.4805533162185124 and best epoch 3\n",
            "--- Epoch 11/100: Training loss = 0.4808, Testing: [loss = 0.4803, dice = 0.4692], Best loss = 0.4803, Best epoch = 11, lr = 0.004846\n",
            "Checkpoint saved at epoch 12 with loss 0.48026969177382334 and best epoch 11\n",
            "--- Epoch 12/100: Training loss = 0.4811, Testing: [loss = 0.4829, dice = 0.4693], Best loss = 0.4803, Best epoch = 11, lr = 0.004829\n",
            "Checkpoint saved at epoch 13 with loss 0.48026969177382334 and best epoch 11\n",
            "--- Epoch 13/100: Training loss = 0.4814, Testing: [loss = 0.4830, dice = 0.4688], Best loss = 0.4803, Best epoch = 11, lr = 0.004811\n",
            "Checkpoint saved at epoch 14 with loss 0.48026969177382334 and best epoch 11\n",
            "--- Epoch 14/100: Training loss = 0.4802, Testing: [loss = 0.4805, dice = 0.4689], Best loss = 0.4803, Best epoch = 11, lr = 0.004794\n",
            "Checkpoint saved at epoch 15 with loss 0.48026969177382334 and best epoch 11\n",
            "--- Epoch 15/100: Training loss = 0.4795, Testing: [loss = 0.4793, dice = 0.4759], Best loss = 0.4793, Best epoch = 15, lr = 0.004776\n",
            "Checkpoint saved at epoch 16 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 16/100: Training loss = 0.4791, Testing: [loss = 0.4833, dice = 0.4713], Best loss = 0.4793, Best epoch = 15, lr = 0.004759\n",
            "Checkpoint saved at epoch 17 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 17/100: Training loss = 0.4800, Testing: [loss = 0.4803, dice = 0.4735], Best loss = 0.4793, Best epoch = 15, lr = 0.004741\n",
            "Checkpoint saved at epoch 18 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 18/100: Training loss = 0.4801, Testing: [loss = 0.4809, dice = 0.4642], Best loss = 0.4793, Best epoch = 15, lr = 0.004724\n",
            "Checkpoint saved at epoch 19 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 19/100: Training loss = 0.4795, Testing: [loss = 0.4841, dice = 0.4725], Best loss = 0.4793, Best epoch = 15, lr = 0.004706\n",
            "Checkpoint saved at epoch 20 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 20/100: Training loss = 0.4791, Testing: [loss = 0.4891, dice = 0.4552], Best loss = 0.4793, Best epoch = 15, lr = 0.004689\n",
            "Checkpoint saved at epoch 21 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 21/100: Training loss = 0.4807, Testing: [loss = 0.4829, dice = 0.4672], Best loss = 0.4793, Best epoch = 15, lr = 0.004671\n",
            "Checkpoint saved at epoch 22 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 22/100: Training loss = 0.4810, Testing: [loss = 0.4823, dice = 0.4624], Best loss = 0.4793, Best epoch = 15, lr = 0.004653\n",
            "Checkpoint saved at epoch 23 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 23/100: Training loss = 0.4802, Testing: [loss = 0.4817, dice = 0.4684], Best loss = 0.4793, Best epoch = 15, lr = 0.004636\n",
            "Checkpoint saved at epoch 24 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 24/100: Training loss = 0.4786, Testing: [loss = 0.4866, dice = 0.4596], Best loss = 0.4793, Best epoch = 15, lr = 0.004618\n",
            "Checkpoint saved at epoch 25 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 25/100: Training loss = 0.4791, Testing: [loss = 0.4820, dice = 0.4683], Best loss = 0.4793, Best epoch = 15, lr = 0.004601\n",
            "Checkpoint saved at epoch 26 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 26/100: Training loss = 0.4787, Testing: [loss = 0.4795, dice = 0.4660], Best loss = 0.4793, Best epoch = 15, lr = 0.004583\n",
            "Checkpoint saved at epoch 27 with loss 0.479275895016534 and best epoch 15\n",
            "--- Epoch 27/100: Training loss = 0.4794, Testing: [loss = 0.4784, dice = 0.4716], Best loss = 0.4784, Best epoch = 27, lr = 0.004566\n",
            "Checkpoint saved at epoch 28 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 28/100: Training loss = 0.4777, Testing: [loss = 0.4847, dice = 0.4661], Best loss = 0.4784, Best epoch = 27, lr = 0.004548\n",
            "Checkpoint saved at epoch 29 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 29/100: Training loss = 0.4768, Testing: [loss = 0.4811, dice = 0.4688], Best loss = 0.4784, Best epoch = 27, lr = 0.004531\n",
            "Checkpoint saved at epoch 30 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 30/100: Training loss = 0.4908, Testing: [loss = 0.4905, dice = 0.4678], Best loss = 0.4784, Best epoch = 27, lr = 0.004513\n",
            "Checkpoint saved at epoch 31 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 31/100: Training loss = 0.4850, Testing: [loss = 0.4847, dice = 0.4618], Best loss = 0.4784, Best epoch = 27, lr = 0.004495\n",
            "Checkpoint saved at epoch 32 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 32/100: Training loss = 0.4828, Testing: [loss = 0.4809, dice = 0.4669], Best loss = 0.4784, Best epoch = 27, lr = 0.004478\n",
            "Checkpoint saved at epoch 33 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 33/100: Training loss = 0.4808, Testing: [loss = 0.4819, dice = 0.4717], Best loss = 0.4784, Best epoch = 27, lr = 0.004460\n",
            "Checkpoint saved at epoch 34 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 34/100: Training loss = 0.4799, Testing: [loss = 0.4847, dice = 0.4690], Best loss = 0.4784, Best epoch = 27, lr = 0.004442\n",
            "Checkpoint saved at epoch 35 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 35/100: Training loss = 0.4782, Testing: [loss = 0.4800, dice = 0.4707], Best loss = 0.4784, Best epoch = 27, lr = 0.004425\n",
            "Checkpoint saved at epoch 36 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 36/100: Training loss = 0.4770, Testing: [loss = 0.4842, dice = 0.4604], Best loss = 0.4784, Best epoch = 27, lr = 0.004407\n",
            "Checkpoint saved at epoch 37 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 37/100: Training loss = 0.4766, Testing: [loss = 0.4818, dice = 0.4689], Best loss = 0.4784, Best epoch = 27, lr = 0.004390\n",
            "Checkpoint saved at epoch 38 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 38/100: Training loss = 0.4753, Testing: [loss = 0.4919, dice = 0.4570], Best loss = 0.4784, Best epoch = 27, lr = 0.004372\n",
            "Checkpoint saved at epoch 39 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 39/100: Training loss = 0.4763, Testing: [loss = 0.4842, dice = 0.4667], Best loss = 0.4784, Best epoch = 27, lr = 0.004354\n",
            "Checkpoint saved at epoch 40 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 40/100: Training loss = 0.4742, Testing: [loss = 0.4823, dice = 0.4659], Best loss = 0.4784, Best epoch = 27, lr = 0.004337\n",
            "Checkpoint saved at epoch 41 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 41/100: Training loss = 0.4733, Testing: [loss = 0.4832, dice = 0.4633], Best loss = 0.4784, Best epoch = 27, lr = 0.004319\n",
            "Checkpoint saved at epoch 42 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 42/100: Training loss = 0.4727, Testing: [loss = 0.4849, dice = 0.4649], Best loss = 0.4784, Best epoch = 27, lr = 0.004301\n",
            "Checkpoint saved at epoch 43 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 43/100: Training loss = 0.4712, Testing: [loss = 0.4811, dice = 0.4671], Best loss = 0.4784, Best epoch = 27, lr = 0.004283\n",
            "Checkpoint saved at epoch 44 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 44/100: Training loss = 0.4707, Testing: [loss = 0.4831, dice = 0.4688], Best loss = 0.4784, Best epoch = 27, lr = 0.004266\n",
            "Checkpoint saved at epoch 45 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 45/100: Training loss = 0.4697, Testing: [loss = 0.4861, dice = 0.4724], Best loss = 0.4784, Best epoch = 27, lr = 0.004248\n",
            "Checkpoint saved at epoch 46 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 46/100: Training loss = 0.4689, Testing: [loss = 0.4845, dice = 0.4602], Best loss = 0.4784, Best epoch = 27, lr = 0.004230\n",
            "Checkpoint saved at epoch 47 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 47/100: Training loss = 0.4690, Testing: [loss = 0.4870, dice = 0.4662], Best loss = 0.4784, Best epoch = 27, lr = 0.004213\n",
            "Checkpoint saved at epoch 48 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 48/100: Training loss = 0.4678, Testing: [loss = 0.4895, dice = 0.4695], Best loss = 0.4784, Best epoch = 27, lr = 0.004195\n",
            "Checkpoint saved at epoch 49 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 49/100: Training loss = 0.4650, Testing: [loss = 0.4869, dice = 0.4644], Best loss = 0.4784, Best epoch = 27, lr = 0.004177\n",
            "Checkpoint saved at epoch 50 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 50/100: Training loss = 0.4630, Testing: [loss = 0.4834, dice = 0.4621], Best loss = 0.4784, Best epoch = 27, lr = 0.004159\n",
            "Checkpoint saved at epoch 51 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 51/100: Training loss = 0.4608, Testing: [loss = 0.4926, dice = 0.4590], Best loss = 0.4784, Best epoch = 27, lr = 0.004142\n",
            "Checkpoint saved at epoch 52 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 52/100: Training loss = 0.4610, Testing: [loss = 0.4933, dice = 0.4577], Best loss = 0.4784, Best epoch = 27, lr = 0.004124\n",
            "Checkpoint saved at epoch 53 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 53/100: Training loss = 0.4599, Testing: [loss = 0.4864, dice = 0.4647], Best loss = 0.4784, Best epoch = 27, lr = 0.004106\n",
            "Checkpoint saved at epoch 54 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 54/100: Training loss = 0.5067, Testing: [loss = 0.5026, dice = 0.4424], Best loss = 0.4784, Best epoch = 27, lr = 0.004088\n",
            "Checkpoint saved at epoch 55 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 55/100: Training loss = 0.5031, Testing: [loss = 0.5027, dice = 0.4473], Best loss = 0.4784, Best epoch = 27, lr = 0.004070\n",
            "Checkpoint saved at epoch 56 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 56/100: Training loss = 0.5031, Testing: [loss = 0.5025, dice = 0.4458], Best loss = 0.4784, Best epoch = 27, lr = 0.004053\n",
            "Checkpoint saved at epoch 57 with loss 0.4784442292792456 and best epoch 27\n",
            "--- Epoch 57/100: Training loss = 0.5032, Testing: [loss = 0.5026, dice = 0.4419], Best loss = 0.4784, Best epoch = 27, lr = 0.004035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cqs19OGiPWfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}